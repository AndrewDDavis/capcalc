#!/usr/bin/env python
#
#   Copyright 2016 Blaise Frederick
#
#   Licensed under the Apache License, Version 2.0 (the "License");
#   you may not use this file except in compliance with the License.
#   You may obtain a copy of the License at
#
#       http://www.apache.org/licenses/LICENSE-2.0
#
#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS,
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#   See the License for the specific language governing permissions and
#   limitations under the License.
#
#
#       $Author: frederic $
#       $Date: 2016/06/14 12:04:51 $
#       $Id: showstxcorr,v 1.11 2016/06/14 12:04:51 frederic Exp $
#
from __future__ import print_function, division
import sys
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import getopt
import rapidtide.tide_funcs as tide
from sklearn.cluster import KMeans, MiniBatchKMeans, DBSCAN
from sklearn.decomposition import PCA, IncrementalPCA, FastICA
from sklearn.ensemble import GradientBoostingRegressor
from sklearn import metrics
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectFdr, SelectKBest, SelectPercentile, f_classif, RFE
from scipy import sin, arange, pi, randn
import numpy as np
import nibabel as nib
try:
    import hdbscan as hdbs
    hdbpresent = True
    print('hdbscan is present')
except:
    hdbpresent = False

def statestats(thestates, numlabels, minlabel):
    # returns statestats and transmat
    #
    # statestats file columns:
    #     percentage of TRs in state
    #     number of continuous runs in state
    #     total number of TRs in state
    #     minimum number of TRs in state
    #     maximum number of TRs in state
    #     average number of TRs in state
    #     median number of TRs in state
    #     standard deviation of the number of TRs in state
    #
    # transmat contains an n_states by n_states matrix:
    #     the number of transitions from state a to state b is in location [a, b]
    #
    minlabel = minlabel
    maxlabel = minlabel + numlabels - 1
    numlabels = maxlabel - minlabel + 1
    transmat = np.zeros((numlabels, numlabels), dtype='float')
    currentstate = thestates[0]
    currentlen = 1
    lenlist = [[]]
    for i in range(numlabels - 1):
        lenlist.append([])
    for state in range(1, len(thestates)):
        if thestates[state] == currentstate:
            currentlen += 1
        else:
            lenlist[currentstate - minlabel].append(currentlen)
            currentstate = thestates[state]
            currentlen = 1
        sourcestate = thestates[state - 1] - minlabel
        deststate = thestates[state] - minlabel
        transmat[sourcestate, deststate] += 1.0
    lenlist[currentstate - minlabel].append(currentlen)
    thestats = []
    for i in range(numlabels):
        lenarray = np.asarray(lenlist[i], dtype='float')
        if len(lenarray) > 2:
            thestats.append([100.0 * np.sum(lenarray)/len(thestates), len(lenarray), np.sum(lenarray), np.min(lenarray), np.max(lenarray), np.mean(lenarray), np.median(lenarray), np.std(lenarray)])
        elif len(lenarray) > 1:
            thestats.append([100.0 * np.sum(lenarray)/len(thestates), len(lenarray), np.sum(lenarray), np.min(lenarray), np.max(lenarray), np.mean(lenarray), lenarray[1], 0.0])
        elif len(lenarray) > 0:
            thestats.append([100.0 * np.sum(lenarray)/len(thestates), len(lenarray), np.sum(lenarray), lenarray[0], lenarray[0], lenarray[0], lenarray[0], 0.0])
        else:
            thestats.append([0.0, len(lenarray), 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
    return transmat, np.asarray(thestats, dtype='float')


def silhouette_test(X, kmeans, n_clusters, numsegs, segsize, summaryonly, display=False):
    print('generating cluster labels')
    cluster_labels = kmeans.fit_predict(X)
    thesilavgs = np.zeros(numsegs, dtype='float')
    thesilclusterstats = np.zeros((numsegs, 4, n_clusters), dtype='float')
    print('calculating silhouette stats')
    for segment in range(numsegs):
        seg_X = X[segment * segsize:(segment + 1) * segsize]
        seg_cluster_labels = cluster_labels[segment * segsize:(segment + 1) * segsize]
        # do a quick sanity check to see if all the labels are present
        clusternums = np.zeros(n_clusters, dtype='int')
        for i in range(len(seg_cluster_labels)):
            clusternums[seg_cluster_labels[i]] += 1
        if np.min(clusternums) > 0:
            thesilavgs[segment] = metrics.silhouette_score(seg_X, seg_cluster_labels)
            print('average silhouette score for segment', segment, '=', thesilavgs[segment])

            if not summaryonly:
                print('doing silhouette samples')
                sample_silhouette_values = metrics.silhouette_samples(seg_X, seg_cluster_labels)
                if display:
                    # Create a subplot with 1 row and 2 columns
                    fig, (ax1) = plt.subplots(1, 1)
                    fig.set_size_inches(8, 4.5)
    
                    # The 1st subplot is the silhouette plot
                    # The silhouette coefficient can range from -1, 1 but in this example all
                    # lie within [-0.3, 1]
                    ax1.set_xlim([-0.3, 1])
                    # The (n_clusters+1)*10 is for inserting blank space between silhouette
                    # plots of individual clusters, to demarcate them clearly.
                    ax1.set_ylim([0, len(seg_X) + (n_clusters + 1) * 10])
        
                    y_lower = 10
                for i in range(n_clusters):
                    # Aggregate the silhouette scores for samples belonging to
                    # cluster i, and sort them
                    ith_cluster_silhouette_values = \
                        sample_silhouette_values[seg_cluster_labels == i]
    
                    ith_cluster_silhouette_values.sort()
                    thesilclusterstats[segment, 0, i] = np.mean(ith_cluster_silhouette_values)
                    thesilclusterstats[segment, 1, i] = np.median(ith_cluster_silhouette_values)
                    thesilclusterstats[segment, 2, i] = ith_cluster_silhouette_values[0]
                    thesilclusterstats[segment, 3, i] = ith_cluster_silhouette_values[-1]
    
                    size_cluster_i = ith_cluster_silhouette_values.shape[0]
    
                    if display:
                        y_upper = y_lower + size_cluster_i
                        color = cm.spectral(float(i) / n_clusters)
                        ax1.fill_betweenx(np.arange(y_lower, y_upper),
                                      0, ith_cluster_silhouette_values,
                                      facecolor=color, edgecolor=color, alpha=0.7)
    
                        # Label the silhouette plots with their cluster numbers at the middle
                        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
    
                        # Compute the new y_lower for next plot
                        y_lower = y_upper + 10  # 10 for the 0 samples
    
                if display:
                    ax1.set_title("The silhouette plot for the various clusters.")
                    ax1.set_xlabel("The silhouette coefficient values")
                    ax1.set_ylabel("Cluster label")
    
                    # The vertical line for average silhouette score of all the values
                    ax1.axvline(x=thesilavgs[segment], color="red", linestyle="--")
    
                    ax1.set_yticks([])  # Clear the yaxis labels / ticks
                    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])
                    plt.suptitle(("Silhouette analysis for KMeans clustering on sample data "
                              "with n_clusters = %d" % n_clusters),
                             fontsize=14, fontweight='bold')
    
                    plt.show()
        else:
            print('states are not fully populated - skipping stats')
    return thesilavgs, thesilclusterstats


def usage():
    print("")
    print("capfromtcs - calculate and cluster coactivation patterns for a set of timecourses")
    print("")
    print("usage: capfromtcs -i timecoursefile -o outputfile --samplefreq=FREQ --sampletime=TSTEP")
    print("                  [--nodetrend] [-s STARTTIME] [-D DURATION]")
    print("                  [-F LOWERFREQ,UPPERFREQ[,LOWERSTOP,UPPERSTOP]] [-V] [-L] [-R] [-C]")
    print("                  [-m] [-n NUMCLUSTER] [-b BATCHSIZE] [-S SEGMENTSIZE] [-I INITIALIZATIONS]")
    print("                  [--noscale] [--nonorm] [--pctnorm] [--varnorm] [--stdnorm] [--ppnorm] [--quality]")
    print("                  [--pca] [--ica] [-p NUMCOMPONENTS]")
    print("")
    print("required arguments:")
    print("    -i, --infile=TIMECOURSEFILE  - text file mulitple timeseries")
    print("    -o, --outfile=OUTNAME        - the root name of the output files")
    print("")
    print("    --samplefreq=FREQ            - sample frequency of all timecourses is FREQ ")
    print("           or")
    print("    --sampletime=TSTEP           - time step of all timecourses is TSTEP ")
    print("                                   NB: --samplefreq and --sampletime are two ways to specify")
    print("                                   the same thing.")
    print("")
    print("optional arguments:")
    print("")
    print("  Data selection/partition:")
    print("    -s STARTTIME                 - time of first datapoint to use in seconds in the first file")
    print("    -D DURATION                  - amount of data to use in seconds")
    print(
        '    -S SEGMENTSIZE,[SEGSIZE2,...SEGSIZEN]')
    print('                                 - treat the timecourses as segments of length SEGMENTSIZE for preprocessing.')
    print(
        '    -E SEGTYPE,SEGTYPE2[,...SEGTYPEN]')
    print('                                 - group subsegments for summary statistics.  SEGTYPEs must be consecutive numbers')
    print('                                   starting at 0, and all subsegments must be the same length')
    print('                                   If there are multiple, comma separated numbers, treat these as subsegment lengths.')
    print('                                   Default segmentsize is the entire length')
    print("  Clustering:")
    print(
        '    -m                           - run MiniBatch Kmeans rather than conventional - use with very large datasets')
    print('    -n NUMCLUSTER                - set the number of clusters to NUMCLUSTER (default is 8)')
    print(
        '    -b BATCHSIZE                 - use a batchsize of BATCHSIZE if doing MiniBatch - ignored if not.  Default is 1000')
    print('    --dbscan                     - perform dbscan clustering')
    print('    --hdbscan                    - perform hdbscan clustering')
    print('    -I INITIALIZATIONS           - Restart KMeans INITIALIZATIONS times to find best fit (default is 1000)')
    print("")
    print('  Preprocessing:')
    print("    -F                           - filter data and regressors from LOWERFREQ to UPPERFREQ.")
    print(
        "                                   LOWERSTOP and UPPERSTOP can be specified, or will be calculated automatically")
    print("    -V                           - filter data and regressors to VLF band")
    print("    -L                           - filter data and regressors to LFO band")
    print("    -R                           - filter data and regressors to respiratory band")
    print("    -C                           - filter data and regressors to cardiac band")
    print("    --nodetrend                  - do not detrend the data before correlation")
    print('    --noscale                    - don\'t perform vector magnitude scaling')
    print('    --nonorm                     - don\'t normalize timecourses')
    print('    --pctnorm                    - scale each timecourse to it\'s percentage of the mean')
    print('    --varnorm                    - scale each timecourse to have a variance of 1.0 (default)')
    print('    --stdnorm                    - scale each timecourse to have a standard deviation of 1.0')
    print('    --ppnorm                     - scale each timecourse to have a peak to peak range of 1.0')
    print('    --pca                        - perform PCA dimensionality reduction prior to analysis')
    print('    --ica                        - perform ICA dimensionality reduction prior to analysis')
    print('    -p NUMCOMPONENTS             - set the number of p/ica components to NUMCOMPONENTS (default is 8).  Set to -1 to estimate')
    print('    --noscale                    - do not apply standard scaler befor cluster fitting')
    print("")
    print('  Other:')
    print('    --GBR                        - apply gradient boosting regressor testing on clusters')
    print('    -d                           - display some quality metrics')
    print('    --quality                    - perform a silhouette test to evaluate fit quality')
    print('    -v                           - turn on verbose mode')
    print("")
    return ()


# get the command line parameters
summaryonly = True

# preprocessing options
preprocessingtype = None
dodetrend = True
timenormmethod = 'varnorm'

# clustering/partitioning options
minibatch = False
n_clusters = 8
n_pca = 8
max_iter = 250
n_init = 100
batch_size = 1000
clustertype = 'kmeans'
clustertype = 'kmeans'
connfilename = None
affinity='euclidean'
linkage='ward'
eps = 0.3
min_samples = 100
alpha = 1.0
standardscale = True

duration = 1000000.0
starttime = 0.0
usebutterworthfilter = False
filtorder = 3
verbose = False

doGBR = False
display = False



# scan the command line arguments
try:
    opts, args = getopt.gnu_getopt(sys.argv[1:], "di:o:s:D:F:S:VLRCmn:p:b:I:v", ["infile=", "outfile=",
                                                                              "nodetrend",
                                                                              "dbscan",
                                                                              "hdbscan",
                                                                              "GBR",
                                                                              "pca",
                                                                              "ica",
                                                                              "noscale",
                                                                              "nonorm",
                                                                              "pctnorm",
                                                                              "varnorm",
                                                                              "stdnorm",
                                                                              "ppnorm",
                                                                              "quality",
                                                                              "samplefreq=", "sampletime=", "help"])
except getopt.GetoptError as err:
    # print help information and exit:
    print(str(err))  # will print something like "option -x not recognized"
    usage()
    sys.exit(2)

if len(args) > 1:
    print('capfromtcs takes no unflagged arguments')
    usage()
    exit()

# unset all required arguments
infilename = []
segsize = -1
subsegs = []
sampletime = None
Fs = None
outfilename = None

theprefilter = tide.noncausalfilter()
theprefilter.setbutter(usebutterworthfilter, filtorder)

# set the default characteristics
theprefilter.settype('none')

for o, a in opts:
    if o == '--infile' or o == '-i':
        infilename.append(a)
        if verbose:
            print('will use', infilename[-1], 'as an input file')
    elif o == '--outfile' or o == '-o':
        outfilename = a
        if verbose:
            print('will use', outfilename, 'as output file')
    elif o == '-S':
        for seg in a.split(','):
            subsegs.append(int(seg))
        segsize = np.sum(np.asarray(subsegs))
        print('SUBSEGS:', subsegs)
        if verbose:
            print('Setting segment size to ', segsize)
    elif o == '--samplefreq':
        Fs = float(a)
        sampletime = 1.0 / Fs
        linkchar = '='
        if verbose:
            print('Setting sample frequency to ', Fs)
    elif o == '--sampletime':
        sampletime = float(a)
        Fs = 1.0 / sampletime
        linkchar = '='
        if verbose:
            print('Setting sample time step to ', sampletime)
    elif o == "-display":
        display = True
        if verbose:
            print('will display quality metrics')
    elif o == "--noscale":
        standardscale = False
        if verbose:
            print('will not magnitude scale feature vectors')
    elif o == "--nonorm":
        timenormmethod = 'none'
        if verbose:
            print('will do no normalization')
    elif o == "--pctnorm":
        timenormmethod = 'pctnorm'
        if verbose:
            print('will do percent normalization')
    elif o == "--stdnorm":
        timenormmethod = 'stdnorm'
        if verbose:
            print('will do std dev normalization')
    elif o == "--varnorm":
        timenormmethod = 'varnorm'
        if verbose:
            print('will do variance normalization')
    elif o == "--ppnorm":
        timenormmethod = 'ppnorm'
        if verbose:
            print('will do p-p normalization')
    elif o == "--quality":
        summaryonly = False
        if verbose:
            print('will do silhouette test')
    elif o == "-v":
        verbose = True
        if verbose:
            print('verbose mode enabled')
    elif o == "--ica":
        preprocessingtype = 'ica'
        if verbose:
            print('will perform ica dimensionality reduction step')
    elif o == "--GBR":
        doGBR = True
        if verbose:
            print('will do GBR on clusters')
    elif o == "--pca":
        preprocessingtype = 'pca'
        if verbose:
            print('will perform pca dimensionality reduction step')
    elif o == "--hdbscan":
        clustertype = 'hdbscan'
        if not hdbpresent:
            print('hdbs is not installed, cannot perform hdbscan clustering.  Exiting')
            sys.exit()
        if verbose:
            print('switching to hdbscan clustering')
    elif o == "--dbscan":
        clustertype = 'dbscan'
        if verbose:
            print('switching to dbscan clustering')
    elif o == "--nodetrend":
        dodetrend = False
        if verbose:
            print('disabling detrending')
    elif o == "-D":
        duration = float(a)
        if verbose:
            print('duration set to', duration)
    elif o == "-s":
        starttime = float(a)
        if verbose:
            print('starttime set to', starttime)
    elif o == "-V":
        theprefilter.settype('vlf')
        if verbose:
            print('prefiltering to vlf band')
    elif o == "-L":
        theprefilter.settype('lfo')
        if verbose:
            print('prefiltering to lfo band')
    elif o == "-R":
        theprefilter.settype('resp')
        if verbose:
            print('prefiltering to respiratory band')
    elif o == "-C":
        theprefilter.settype('cardiac')
        if verbose:
            print('prefiltering to cardiac band')
    elif o == "-F":
        arbvec = a.split(',')
        if len(arbvec) != 2 and len(arbvec) != 4:
            usage()
            sys.exit()
        if len(arbvec) == 2:
            arb_lower = float(arbvec[0])
            arb_upper = float(arbvec[1])
            arb_lowerstop = 0.9 * float(arbvec[0])
            arb_upperstop = 1.1 * float(arbvec[1])
        if len(arbvec) == 4:
            arb_lower = float(arbvec[0])
            arb_upper = float(arbvec[1])
            arb_lowerstop = float(arbvec[2])
            arb_upperstop = float(arbvec[3])
        theprefilter.settype('arb')
        theprefilter.setarb(arb_lowerstop, arb_lower, arb_upper, arb_upperstop)
        if verbose:
            print('prefiltering to ', arb_lower, arb_upper, "(stops at ", arb_lowerstop, arb_upperstop, ")")
    elif o == "-m":
        minibatch = True
        print('will perform MiniBatchKMeans')
    elif o == "-b":
        batch_size = int(a)
        print('will use', batch_size, 'as batch_size')
    elif o == "-I":
        n_init = int(a)
        print('will do', n_init, 'initializations')
    elif o == "-n":
        n_clusters = int(a)
        print('will use', n_clusters, 'clusters')
    elif o == "-p":
        n_pca = float(a)
        if n_pca <= 0.0:
            print('will estimate the number of pca components for dimensionality reduction')
        elif n_pca < 1.0:
            print('will use enough pca components to explain at least', 100.0 * n_pca, '% of the variance')
        else:
            n_pca = int(a)
            print('will use', n_pca, 'pca components for dimensionality reduction')
    else:
        assert False, "unhandled option"

# check that required arguments are set
if outfilename is None:
    print('outfile must be set')
    usage()
    sys.exit()

if sampletime is None:
    print('sampletime must be set')
    usage()
    sys.exit()

if timenormmethod == 'none':
    print('will not normalize timecourses')
elif timenormmethod == 'pctnorm':
    print('will normalize timecourses to percentage of mean')
elif timenormmethod == 'stdnorm':
    print('will normalize timecourses to standard deviation of 1.0')
elif timenormmethod == 'varnorm':
    print('will normalize timecourses to variance of 1.0')
elif timenormmethod == 'ppnorm':
    print('will normalize timecourses to p-p deviation of 1.0')

# save the command line
tide.writevec([' '.join(sys.argv)], outfilename + '_commandline.txt')

# read in the files and get everything trimmed to the right length
startpoint = max([int(starttime * Fs), 0])
if len(infilename) == 1:
    # each column is a timecourse, each row is a timepoint
    matrixoutput = True
    inputdata = tide.readvecs(infilename[0])
    if verbose:
        print('input data shape is ', inputdata.shape)
    numpoints = inputdata.shape[1]
    endpoint = min([startpoint + int(duration * Fs), numpoints])
    trimmeddata = inputdata[:, startpoint:endpoint]
elif len(infilename) == 2:
    inputdata1 = tide.readvec(infilename[0])
    numpoints = len(inputdata1)
    inputdata2 = tide.readvec(infilename[1])
    endpoint1 = min([startpoint + int(duration * Fs), int(len(inputdata1)), int(len(inputdata2))])
    endpoint2 = min([int(duration * Fs), int(len(inputdata1)), int(len(inputdata2))])
    trimmeddata = np.zeros((2, numpoints), dtype='float')
    trimmeddata[0, :] = inputdata1[startpoint:endpoint1]
    trimmeddata[1, :] = inputdata2[0:endpoint2]
else:
    print('showstxcorr requires 1 multicolumn timecourse file or two single column timecourse files as input')
    usage()
    sys.exit()

# band limit the regressors if that is needed
if theprefilter.gettype() != 'none':
    if verbose:
        print("filtering to ", theprefilter.gettype(), " band")

thedims = trimmeddata.shape
n_features = thedims[0]
n_samples = thedims[1]
if segsize < 0:
    segsize = n_samples
    subsegs.append(segsize)
print('input dataset has', n_features, 'features and', n_samples, 'samples in segments of size', segsize)
if len(subsegs) > 1:
    print('    segment is broken into', len(subsegs), 'subsegments of length', subsegs)
reformdata = np.reshape(trimmeddata, (n_features, n_samples))
if n_samples % segsize > 0:
    print('segment size is not an even divisor of the total length - exiting')
    sys.exit()
else:
    numsegs = int(n_samples // segsize)

for feature in range(n_features):
    if verbose:
        print('preprocessing feature', feature)
    for segment in range(numsegs):
        subsegstart = segment * segsize
        for subseglen in subsegs:
            if dodetrend:
                segdata = tide.detrend(reformdata[feature, subsegstart:subsegstart + subseglen])
            else:
                segdata = reformdata[feature, subsegstart:subsegstart + subseglen]
    
            if timenormmethod == 'none':
                segnorm = segdata - np.mean(segdata)
            elif timenormmethod == 'pctnorm':
                segnorm = tide.pcnormalize(segdata)
            elif timenormmethod == 'varnorm':
                segnorm = tide.varnormalize(segdata)
            elif timenormmethod == 'stdnorm':
                segnorm = tide.stdnormalize(segdata)
            elif timenormmethod == 'ppnorm':
                segnorm = tide.ppnormalize(segdata)
            else:
                segnorm = segdata

            reformdata[feature, subsegstart:subsegstart + subseglen] = theprefilter.apply(Fs, segnorm)
            subsegstart += subseglen
X = np.nan_to_num(np.transpose(reformdata))

if standardscale:
    X = StandardScaler().fit_transform(X)

if preprocessingtype == 'pca':
    print('running PCA')
    print('shape going in:', X.shape)
    if False:
        if n_pca <=0:
            n_pca = None
        thepca = IncrementalPCA(batch_size=batch_size, n_components=n_pca)
    else:
        if n_pca <=0:
            thepca = PCA(n_components='mle', svd_solver='full')
        else:
            thepca = PCA(n_components=n_pca)
    thefit = thepca.fit(X)
    thetransform = thepca.transform(X)
    X = thepca.inverse_transform(thetransform)
    print('shape coming out:', X.shape)
    for i in range(thepca.n_components_):
        print('component', i, 'explained variance:', thepca.explained_variance_[i], 'explained variance %:',
              100.0 * thepca.explained_variance_ratio_[i])
    tide.writenpvecs(thepca.components_, outfilename + "_pcacomponents.txt")
elif preprocessingtype == 'ica':
    print('running FastICA')
    if n_pca <= 1.0:
        n_pca = int(0)
    theica = FastICA(n_components=n_pca, algorithm='deflation')
    thefit = theica.fit(X)
    thetransform = theica.transform(X)
    X = theica.inverse_transform(thetransform)
    tide.writenpvecs(theica.components_, outfilename + "_icacomponents.txt")

tide.writenpvecs(reformdata, outfilename + '_preprocessed.txt')

if clustertype == 'kmeans':
    print('setting up kmeans')
    if minibatch:
        kmeans = MiniBatchKMeans(n_clusters=n_clusters, batch_size=batch_size, max_iter=max_iter).fit(X)
    else:
        kmeans = KMeans(n_clusters=n_clusters, max_iter=max_iter, n_init=n_init).fit(X)
    
    theclusters = np.transpose(kmeans.cluster_centers_)
    thestatelabels = kmeans.labels_
    print('thestatelabels shape', thestatelabels.shape)
    print('kmeans done')
    tide.writenpvecs(theclusters, outfilename + '_clustercenters.txt')
    tide.writenpvecs(thestatelabels, outfilename + '_statelabels.txt')

    # find most important features
    print('finding most important features')
    #rfe = RFE(kmeans, 10)
    #rfe.fit(X, thestatelabels)
    #print(rfe.support_)
    #print(rfe.ranking_)

    print('calling SelectPercentiles with X and y of dimensions',X.shape, thestatelabels.shape)
    selector = SelectPercentile(f_classif, percentile=10)
    selector.fit(X, thestatelabels)
    print(selector.get_params())
    X_indices = np.arange(X.shape[-1])
    scores = -np.nan_to_num(np.log10(np.nan_to_num(selector.pvalues_)))
    scores /= scores.max()
    sortedscores = np.sort(np.nan_to_num(selector.scores_))[::-1]
    print(sortedscores)
    if display:
        plt.bar(X_indices - .45, scores, width=.2,
            label=r'Univariate score ($-Log(p_{value})$)', color='darkorange')
        print(selector.get_support(indices=True))
        fig = plt.subplots(1, 1)
        plt.plot(sortedscores)
        plt.show()

    # now do some stats!
    thesilavgs, thesilclusterstats = silhouette_test(X, kmeans, n_clusters, numsegs, segsize, summaryonly)
    tide.writenpvecs(thesilavgs, outfilename + '_silhouettesegmentstats.txt')

    silinfo = []
    for state in range(n_clusters):
        silinfo.append([])
    print('shape going in:', thestatelabels.shape)
    statelabelsbysegment = np.reshape(thestatelabels, (-1, segsize))
    print('shape coming out:', statelabelsbysegment.shape)
    meaninstate = np.zeros((n_clusters, segsize), dtype='float')
    stdinstate = np.zeros((n_clusters, segsize), dtype='float')
    for state in range(n_clusters):
        tcbyseg = np.where(statelabelsbysegment == state, 1, 0)
        meaninstate[state, :] = np.mean(tcbyseg, axis=0)
        stdinstate[state, :] = np.std(tcbyseg, axis=0)
    tide.writenpvecs(meaninstate, outfilename + '_meaninstate.txt')
    tide.writenpvecs(stdinstate, outfilename + '_stdinstate.txt')
    for segment in range(numsegs):
        thesestatelabels = thestatelabels[segment * segsize:(segment + 1) * segsize]

        outputaffine = np.eye(4)
        rawtransmat, thestats = statestats(thesestatelabels, n_clusters, 0)
        normtransmat = 1.0 * rawtransmat
        for i in range(n_clusters):
            if np.sum(rawtransmat[i, :]) > 0.0:
                normtransmat[i, :] /= np.sum(rawtransmat[i, :])
        offdiagtransmat = 1.0 * rawtransmat
        for i in range(n_clusters):
            offdiagtransmat[i, i] = 0.0
            if np.sum(offdiagtransmat[i, :]) > 0.0:
                offdiagtransmat[i, :] /= np.sum(offdiagtransmat[i, :])
        init_img=nib.Nifti1Image(normtransmat, outputaffine)
        init_hdr=init_img.get_header()
        init_sizes=init_hdr['pixdim']
        tide.savetonifti(rawtransmat, init_hdr, init_sizes, outfilename + '_seg_' + str(segment).zfill(4) + '_rawtransmat')
        tide.savetonifti(normtransmat, init_hdr, init_sizes, outfilename + '_seg_' + str(segment).zfill(4) + '_normtransmat')
        tide.savetonifti(offdiagtransmat, init_hdr, init_sizes, outfilename + '_seg_' + str(segment).zfill(4) + '_offdiagtransmat')
        # rawtransmat files are an n_clusters by n_clusters matrix with the total number of transitions from each state to each other state.
        # normtransmat files are an n_clusters by n_clusters matrix with the total for of transitions from each state to each other state.

        tide.writenpvecs(np.transpose(thestats), outfilename + '_seg_' + str(segment).zfill(4) + '_statestats.txt')
        thetimestats = 1.0 * thestats
        thetimestats[:, 2:] *= sampletime
        tide.writenpvecs(np.transpose(thetimestats), outfilename + '_seg_' + str(segment).zfill(4) + '_statetimestats.txt')

        tide.writenpvecs(thesestatelabels, outfilename + '_seg_' + str(segment).zfill(4) + '_statelabels.txt')
        print('Segment %d average silhouette Coefficient: %0.3f'
              % (segment, thesilavgs[segment]))
        for state in range(n_clusters):
            tc = np.where(thesestatelabels == state, 1, 0)
            tide.writenpvecs(tc,
                             outfilename + '_seg_' + str(segment).zfill(4) + '_instate_' + str(state).zfill(2) + '.txt')
        tide.writenpvecs(thesilclusterstats[segment, :, :],
                         outfilename + '_seg_' + str(segment).zfill(4) + '_silhouetteclusterstats.txt')

        for state in range(n_clusters):
            if thestats[state, 2] > 0:
                silinfo[state].append(thesilclusterstats[segment, 0, state])

    # now generate some summary information
    silavgs = []
    subjpcts = []
    for state in range(n_clusters):
        silavgs.append(np.mean(np.asarray(silinfo[state], dtype='float')))
        subjpcts.append(100.0 * len(silinfo[state]) / numsegs)
    tide.writenpvecs(np.asarray(silavgs, dtype='float'), outfilename + '_overallsilhouettemean.txt')
    tide.writenpvecs(np.asarray(subjpcts, dtype='float'), outfilename + '_pctsegsinstate.txt')
    
    if doGBR:
        clf = GradientBoostingRegressor().fit(X, thestatelabels)
        print('GBR fitting score is:', clf.score(X, thestatelabels))
        tide.writenpvecs(np.reshape(clf.feature_importances_, (n_features, 1)), outfilename + '_featureimportances.txt')

elif clustertype == 'dbscan':
    db = DBSCAN(eps=eps, min_samples=min_samples, n_jobs=-1).fit(X)
    print('dbscan done')

    #core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
    #core_samples_mask[db.core_sample_indices_] = True

    thestatelabels = db.labels_
    print(thestatelabels)
    print('thestatelabels shape', thestatelabels.shape)
    tide.writenpvecs(thestatelabels, outfilename + '_statelabels.txt')

    print('core_sample_indices:', db.core_sample_indices_)
    core_centers = np.transpose(X[db.core_sample_indices_, :])
    tide.writenpvecs(core_centers, outfilename + '_core_centers.txt')

    # Number of clusters in labels, ignoring noise if present.
    n_clusters_ = len(set(thestatelabels)) - (1 if -1 in thestatelabels else 0)
    print('Estimated number of clusters: %d' % n_clusters_)

    methodname = 'dbscan_' + str(n_clusters).zfill(2)

elif clustertype == 'hdbscan':
    hdb = hdbs.HDBSCAN(min_samples=min_samples,
                        alpha=alpha, 
                        memory='/Users/frederic/Documents/MR_data/connectome/movies').fit(X)
    thestatelabels = hdb.labels_
    print(thestatelabels)

    # Number of clusters in labels, ignoring noise if present.
    n_clusters_ = len(set(thestatelabels)) - (1 if -1 in thestatelabels else 0)

    print('Estimated number of clusters: %d' % n_clusters_)
    methodname = 'hdbscan_' + str(n_clusters).zfill(2)

else:  
    print('unknown clustering type')
    sys.exit()
