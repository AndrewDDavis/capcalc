#!/usr/bin/env python
#
#   Copyright 2016 Blaise Frederick
#
#   Licensed under the Apache License, Version 2.0 (the "License");
#   you may not use this file except in compliance with the License.
#   You may obtain a copy of the License at
#
#       http://www.apache.org/licenses/LICENSE-2.0
#
#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS,
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#   See the License for the specific language governing permissions and
#   limitations under the License.
#
#
#       $Author: frederic $
#       $Date: 2016/06/14 12:04:51 $
#       $Id: showstxcorr,v 1.11 2016/06/14 12:04:51 frederic Exp $
#
from __future__ import print_function, division
import sys
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import getopt
import rapidtide.tide_funcs as tide
from sklearn.cluster import KMeans, MiniBatchKMeans
from sklearn.metrics import silhouette_samples, silhouette_score
from scipy import sin, arange, pi, randn
import numpy as np
import nibabel as nib

def silhouette_test(X, kmeans, n_clusters, outfilename):
    cluster_labels = kmeans.fit_predict(X)
    silhouette_avg = silhouette_score(X, kmeans.labels_)
    sample_silhouette_values = silhouette_samples(X, kmeans.labels_)

    # Create a subplot with 1 row and 2 columns
    fig, (ax1) = plt.subplots(1, 1)
    fig.set_size_inches(8, 4.5)

    # The 1st subplot is the silhouette plot
    # The silhouette coefficient can range from -1, 1 but in this example all
    # lie within [-0.3, 1]
    ax1.set_xlim([-0.3, 1])
    # The (n_clusters+1)*10 is for inserting blank space between silhouette
    # plots of individual clusters, to demarcate them clearly.
    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])

    thesilstats = np.zeros((n_clusters, 3), dtype='float')
    y_lower = 10
    for i in range(n_clusters):
        # Aggregate the silhouette scores for samples belonging to
        # cluster i, and sort them
        ith_cluster_silhouette_values = \
            sample_silhouette_values[cluster_labels == i]

        ith_cluster_silhouette_values.sort()
        thesilstats[i, 0] = np.mean(ith_cluster_silhouette_values)
        thesilstats[i, 1] = ith_cluster_silhouette_values[0]
        thesilstats[i, 2] = ith_cluster_silhouette_values[-1]

        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        color = cm.spectral(float(i) / n_clusters)
        ax1.fill_betweenx(np.arange(y_lower, y_upper),
                          0, ith_cluster_silhouette_values,
                          facecolor=color, edgecolor=color, alpha=0.7)

        # Label the silhouette plots with their cluster numbers at the middle
        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

        # Compute the new y_lower for next plot
        y_lower = y_upper + 10  # 10 for the 0 samples

    ax1.set_title("The silhouette plot for the various clusters.")
    ax1.set_xlabel("The silhouette coefficient values")
    ax1.set_ylabel("Cluster label")

    # The vertical line for average silhouette score of all the values
    ax1.axvline(x=silhouette_avg, color="red", linestyle="--")

    ax1.set_yticks([])  # Clear the yaxis labels / ticks
    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])
    plt.suptitle(("Silhouette analysis for KMeans clustering on sample data "
                  "with n_clusters = %d" % n_clusters),
                 fontsize=14, fontweight='bold')

    plt.show()
    tide.writenpvecs(thesilstats, outfilename + "_silhouettestats.txt")


def usage():
    print("")
    print("capfromtcs - calculate and cluster coactivation patterns for a set of timecourses")
    print("")
    print("usage: capfromtcs -i timecoursefile -o outputfile --samplefreq=FREQ --sampletime=TSTEP")
    print("                  [--nodetrend] [-s STARTTIME] [-D DURATION]")
    print("                  [-F LOWERFREQ,UPPERFREQ[,LOWERSTOP,UPPERSTOP]] [-V] [-L] [-R] [-C]")
    print("                  [-m] [-n NUMCLUSTER] [-b BATCHSIZE] [-S SEGMENTSIZE] [-I INITIALIZATIONS]")
    print("                  [--nonorm] [--pctnorm] [--varnorm] [--stdnorm] [--ppnorm] [--quality]")
    print("")
    print("required arguments:")
    print("    -i, --infile=TIMECOURSEFILE  - text file mulitple timeseries")
    print("    -o, --outfile=OUTNAME        - the root name of the output files")
    print("")
    print("    --samplefreq=FREQ            - sample frequency of all timecourses is FREQ ")
    print("           or")
    print("    --sampletime=TSTEP           - time step of all timecourses is TSTEP ")
    print("                                   NB: --samplefreq and --sampletime are two ways to specify")
    print("                                   the same thing.")
    print("")
    print("optional arguments:")
    print("    --nodetrend                  - do not detrend the data before correlation")
    print("    -s STARTTIME                 - time of first datapoint to use in seconds in the first file")
    print("    -D DURATION                  - amount of data to use in seconds")
    print("    -F                           - filter data and regressors from LOWERFREQ to UPPERFREQ.")
    print("                                   LOWERSTOP and UPPERSTOP can be specified, or will be calculated automatically")
    print("    -V                           - filter data and regressors to VLF band")
    print("    -L                           - filter data and regressors to LFO band")
    print("    -R                           - filter data and regressors to respiratory band")
    print("    -C                           - filter data and regressors to cardiac band")
    print('    -m                           - run MiniBatch Kmeans rather than conventional - use with very large datasets')
    print('    -n NUMCLUSTER                - set the number of clusters to NUMCLUSTER (default is 8)')
    print('    -b BATCHSIZE                 - use a batchsize of BATCHSIZE if doing MiniBatch - ignored if not.  Default is 100')
    print('    -S SEGMENTSIZE               - treat the timecourses as segments of length SEGMENTSIZE for preprocessing.')
    print('                                   Default segmentsize is the entire length')
    print('    -I INITIALIZATIONS           - Restart KMeans INITIALIZATIONS times to find best fit (default is 1000)')
    print('    --nonorm                     - don\'t normalize timecourses')
    print('    --pctnorm                    - scale each timecourse to it\'s percentage of the mean')
    print('    --varnorm                    - scale each timecourse to have a variance of 1.0')
    print('    --stdnorm                    - scale each timecourse to have a standard deviation of 1.0 (default)')
    print('    --ppnorm                     - scale each timecourse to have a peak to peak range of 1.0')
    print('    --quality                    - perform a silhouette test to evaluate fit quality')
    print('    -v                           - turn on verbose mode')
    print("")
    return ()


# get the command line parameters
doquality = False
dodetrend = True
normmethod = 'stdnorm'
minibatch = False
n_clusters = 8
max_iter = 250
n_init = 100
batch_size = 100
duration = 1000000.0
starttime = 0.0
usebutterworthfilter = False
filtorder = 3
verbose = False

# scan the command line arguments
try:
    opts, args = getopt.gnu_getopt(sys.argv[1:], "i:o:s:D:F:S:VLRCmn:b:I:v", ["infile=", "outfile=",
                                                                                    "nodetrend",
                                                                                    "nonorm",
                                                                                    "pctnorm",
                                                                                    "varnorm",
                                                                                    "stdnorm",
                                                                                    "ppnorm",
                                                                                    "quality",
                                                                                    "samplefreq=", "sampletime=", "help"])
except getopt.GetoptError as err:
    # print help information and exit:
    print(str(err))  # will print something like "option -x not recognized"
    usage()
    sys.exit(2)

if len(args) > 1:
    print('capfromtcs takes no unflagged arguments')
    usage()
    exit()

# unset all required arguments
infilename = []
segsize = -1
sampletime = None
Fs = None
outfilename = None

theprefilter = tide.noncausalfilter()
theprefilter.setbutter(usebutterworthfilter, filtorder)

# set the default characteristics
theprefilter.settype('none')

for o, a in opts:
    if o == '--infile' or o == '-i':
        infilename.append(a)
        if verbose:
            print('will use', infilename[-1], 'as an input file')
    elif o == '--outfile' or o == '-o':
        outfilename = a
        if verbose:
            print('will use', outfilename, 'as output file')
    elif o == '-S':
        segsize = int(a)
        if verbose:
            print('Setting segment size to ', segsize)
    elif o == '--samplefreq':
        Fs = float(a)
        sampletime = 1.0 / Fs
        linkchar = '='
        if verbose:
            print('Setting sample frequency to ', Fs)
    elif o == '--sampletime':
        sampletime = float(a)
        Fs = 1.0 / sampletime
        linkchar = '='
        if verbose:
            print('Setting sample time step to ', sampletime)
    elif o == "--nonorm":
        normmethod = 'none'
        if verbose:
            print('will do no normalization')
    elif o == "--pctnorm":
        normmethod = 'pctnorm'
        if verbose:
            print('will do percent normalization')
    elif o == "--stdnorm":
        normmethod = 'stdnorm'
        if verbose:
            print('will do std dev normalization')
    elif o == "--varnorm":
        normmethod = 'varnorm'
        if verbose:
            print('will do variance normalization')
    elif o == "--ppnorm":
        normmethod = 'ppnorm'
        if verbose:
            print('will do p-p normalization')
    elif o == "--quality":
        doquality = True
        if verbose:
            print('will do silhouette test')
    elif o == "-v":
        verbose = True
        if verbose:
            print('verbose mode enabled')
    elif o == "--nodetrend":
        dodetrend = False
        if verbose:
            print('disabling detrending')
    elif o == "-D":
        duration = float(a)
        if verbose:
            print('duration set to', duration)
    elif o == "-s":
        starttime = float(a)
        if verbose:
            print('starttime set to', starttime)
    elif o == "-V":
        theprefilter.settype('vlf')
        if verbose:
            print('prefiltering to vlf band')
    elif o == "-L":
        theprefilter.settype('lfo')
        if verbose:
            print('prefiltering to lfo band')
    elif o == "-R":
        theprefilter.settype('resp')
        if verbose:
            print('prefiltering to respiratory band')
    elif o == "-C":
        theprefilter.settype('cardiac')
        if verbose:
            print('prefiltering to cardiac band')
    elif o == "-F":
        arbvec = a.split(',')
        if len(arbvec) != 2 and len(arbvec) != 4:
            usage()
            sys.exit()
        if len(arbvec) == 2:
            arb_lower = float(arbvec[0])
            arb_upper = float(arbvec[1])
            arb_lowerstop = 0.9 * float(arbvec[0])
            arb_upperstop = 1.1 * float(arbvec[1])
        if len(arbvec) == 4:
            arb_lower = float(arbvec[0])
            arb_upper = float(arbvec[1])
            arb_lowerstop = float(arbvec[2])
            arb_upperstop = float(arbvec[3])
        theprefilter.settype('arb')
        theprefilter.setarb(arb_lowerstop, arb_lower, arb_upper, arb_upperstop)
        if verbose:
            print('prefiltering to ', arb_lower, arb_upper, "(stops at ", arb_lowerstop, arb_upperstop, ")")
    elif o == "-m":
        minibatch = True
        print('will perform MiniBatchKMeans')
    elif o == "-b":
        batch_size = int(a)
        print('will use', batch_size, 'as batch_size (if doing MiniBatchKMeans)')
    elif o == "-I":
        n_init = int(a)
        print('will do', n_init, 'initializations')
    elif o == "-n":
        n_clusters = int(a)
        print('will use', n_clusters, 'clusters')
    else:
        assert False, "unhandled option"

# check that required arguments are set
if outfilename is None:
    print('outfile must be set')
    usage()
    sys.exit()

if sampletime is None:
    print('sampletime must be set')
    usage()
    sys.exit()

if normmethod == 'none':
    print('will not normalize timecourses')
elif normmethod == 'pctnorm':
    print('will normalize timecourses to percentage of mean')
elif normmethod == 'stdnorm':
    print('will normalize timecourses to standard deviation of 1.0')
elif normmethod == 'varnorm':
    print('will normalize timecourses to variance of 1.0')
elif normmethod == 'ppnorm':
    print('will normalize timecourses to p-p deviation of 1.0')


# read in the files and get everything trimmed to the right length
startpoint = max([int(starttime * Fs), 0])
if len(infilename) == 1:
    # each column is a timecourse, each row is a timepoint
    matrixoutput = True
    inputdata = tide.readvecs(infilename[0])
    if verbose:
        print('input data shape is ', inputdata.shape)
    numpoints = inputdata.shape[1]
    endpoint = min([startpoint + int(duration * Fs), numpoints])
    trimmeddata = inputdata[:, startpoint:endpoint]
elif len(infilename) == 2:
    inputdata1 = tide.readvec(infilename[0])
    numpoints = len(inputdata1)
    inputdata2 = tide.readvec(infilename[1])
    endpoint1 = min([startpoint + int(duration * Fs), int(len(inputdata1)), int(len(inputdata2))])
    endpoint2 = min([int(duration * Fs), int(len(inputdata1)), int(len(inputdata2))])
    trimmeddata = np.zeros((2, numpoints), dtype='float')
    trimmeddata[0, :] = inputdata1[startpoint:endpoint1]
    trimmeddata[1, :] = inputdata2[0:endpoint2]
else:
    print('showstxcorr requires 1 multicolumn timecourse file or two single column timecourse files as input')
    usage()
    sys.exit()
    
# band limit the regressors if that is needed
if theprefilter.gettype() != 'none':
    if verbose:
        print("filtering to ", theprefilter.gettype(), " band")

thedims=trimmeddata.shape
numcomponents=thedims[0]
tclen=thedims[1]
reformdata=np.reshape(trimmeddata,(numcomponents,tclen))
if segsize < 0:
    segsize = tclen
if tclen % segsize > 0:
    print('segment size is not an even divisor of the total length - exiting')
    sys.exit()
else:
    numsegs = int(tclen // segsize)
 
for component in range(0,numcomponents):
    if verbose:
        print('preprocessing component', component)
    for segment in range(numsegs):
        segstart = segment * segsize
        if dodetrend:
            segdata = tide.detrend(reformdata[component,segstart:segstart + segsize])
        else:
            segdata = reformdata[component,segstart:segstart + segsize]
        if normmethod == 'none':
            segnorm = segdata - np.mean(segdata)
        elif normmethod == 'pctnorm':
            segnorm = tide.pcnormalize(segdata)
        elif normmethod == 'varnorm':
            segnorm = tide.varnormalize(segdata)
        elif normmethod == 'stdnorm':
            segnorm = tide.stdnormalize(segdata)
        elif normmethod == 'ppnorm':
            segnorm = tide.ppnormalize(segdata)
        reformdata[component,segstart:segstart + segsize] = theprefilter.apply(Fs, segnorm)
tide.writenpvecs(reformdata, outfilename + '_normalized.txt')
X = np.nan_to_num(np.transpose(reformdata))

print('setting up kmeans')

if minibatch:
    kmeans = MiniBatchKMeans(n_clusters=n_clusters, batch_size=batch_size, max_iter=max_iter).fit(X)
else:
    kmeans = KMeans(n_clusters=n_clusters, max_iter=max_iter, n_init=n_init).fit(X)

if doquality:
    silhouette_test(X, kmeans, n_clusters, outfilename)

theclusters = np.transpose(kmeans.cluster_centers_)
thestatelabels = kmeans.labels_

# now do some stats!
for segment in range(numsegs):
    print('calculating stats for segment', segment)
    for state in range(n_clusters):
        tc = np.where(thestatelabels == state, 1, 0) 
        tide.writenpvecs(tc, outfilename + '_instate_' + str(state).zfill(2) + '.txt')

tide.writenpvecs(theclusters, outfilename + "_clustercenters.txt")
tide.writenpvecs(thestatelabels, outfilename + '_statelabels.txt')
