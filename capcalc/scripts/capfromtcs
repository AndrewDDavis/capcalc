#!/usr/bin/env python
#
#   Copyright 2016 Blaise Frederick
#
#   Licensed under the Apache License, Version 2.0 (the "License");
#   you may not use this file except in compliance with the License.
#   You may obtain a copy of the License at
#
#       http://www.apache.org/licenses/LICENSE-2.0
#
#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS,
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#   See the License for the specific language governing permissions and
#   limitations under the License.
#
#
#       $Author: frederic $
#       $Date: 2016/06/14 12:04:51 $
#       $Id: showstxcorr,v 1.11 2016/06/14 12:04:51 frederic Exp $
#
from __future__ import print_function, division
import sys
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import getopt
import rapidtide.tide_funcs as tide
from sklearn.cluster import KMeans, MiniBatchKMeans, DBSCAN
from sklearn.decomposition import PCA, IncrementalPCA, FastICA
from sklearn.ensemble import GradientBoostingRegressor
from sklearn import metrics
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectFdr, SelectKBest, SelectPercentile, f_classif, RFE
from scipy import sin, arange, pi, randn
import numpy as np
import nibabel as nib


def statestats(thestates, numlabels, minlabel):
    # returns statestats and transmat
    #
    # statestats file columns:
    #     percentage of TRs in state
    #     number of continuous runs in state
    #     total number of TRs in state
    #     minimum number of TRs in state
    #     maximum number of TRs in state
    #     average number of TRs in state
    #     median number of TRs in state
    #     standard deviation of the number of TRs in state
    #
    # transmat contains an n_states by n_states matrix:
    #     the number of transitions from state a to state b is in location [a, b]
    #
    minlabel = minlabel
    maxlabel = minlabel + numlabels - 1
    numlabels = maxlabel - minlabel + 1
    transmat = np.zeros((numlabels, numlabels), dtype='float')
    currentstate = thestates[0]
    currentlen = 1
    lenlist = [[]]
    for i in range(numlabels - 1):
        lenlist.append([])
    for state in range(1, len(thestates)):
        if thestates[state] == currentstate:
            currentlen += 1
        else:
            lenlist[currentstate - minlabel].append(currentlen)
            currentstate = thestates[state]
            currentlen = 1
        sourcestate = thestates[state - 1] - minlabel
        deststate = thestates[state] - minlabel
        transmat[sourcestate, deststate] += 1.0
    lenlist[currentstate - minlabel].append(currentlen)
    thestats = []
    for i in range(numlabels):
        lenarray = np.asarray(lenlist[i], dtype='float')
        if len(lenarray) > 2:
            thestats.append([100.0 * np.sum(lenarray)/len(thestates), len(lenarray), np.sum(lenarray), np.min(lenarray), np.max(lenarray), np.mean(lenarray), np.median(lenarray), np.std(lenarray)])
        elif len(lenarray) > 1:
            thestats.append([100.0 * np.sum(lenarray)/len(thestates), len(lenarray), np.sum(lenarray), np.min(lenarray), np.max(lenarray), np.mean(lenarray), lenarray[1], 0.0])
        elif len(lenarray) > 0:
            thestats.append([100.0 * np.sum(lenarray)/len(thestates), len(lenarray), np.sum(lenarray), lenarray[0], lenarray[0], lenarray[0], lenarray[0], 0.0])
        else:
            thestats.append([0.0, len(lenarray), 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
    return transmat, np.asarray(thestats, dtype='float')


def silhouette_test(X, kmeans, n_clusters, numsegs, segsize, summaryonly, display=False):
    print('generating cluster labels')
    cluster_labels = kmeans.fit_predict(X)
    thesilavgs = np.zeros(numsegs, dtype='float')
    thesilclusterstats = np.zeros((numsegs, 4, n_clusters), dtype='float')
    print('calculating silhouette stats')
    for segment in range(numsegs):
        seg_X = X[segment * segsize:(segment + 1) * segsize]
        seg_cluster_labels = cluster_labels[segment * segsize:(segment + 1) * segsize]
        # do a quick sanity check to see if all the labels are present
        clusternums = np.zeros(n_clusters, dtype='int')
        for i in range(len(seg_cluster_labels)):
            clusternums[seg_cluster_labels[i]] += 1
        if np.min(clusternums) > 0:
            thesilavgs[segment] = metrics.silhouette_score(seg_X, seg_cluster_labels)
            print('average silhouette score for segment', segment, '=', thesilavgs[segment])

            if not summaryonly:
                print('doing silhouette samples')
                sample_silhouette_values = metrics.silhouette_samples(seg_X, seg_cluster_labels)
                if display:
                    # Create a subplot with 1 row and 2 columns
                    fig, (ax1) = plt.subplots(1, 1)
                    fig.set_size_inches(8, 4.5)
    
                    # The 1st subplot is the silhouette plot
                    # The silhouette coefficient can range from -1, 1 but in this example all
                    # lie within [-0.3, 1]
                    ax1.set_xlim([-0.3, 1])
                    # The (n_clusters+1)*10 is for inserting blank space between silhouette
                    # plots of individual clusters, to demarcate them clearly.
                    ax1.set_ylim([0, len(seg_X) + (n_clusters + 1) * 10])
        
                    y_lower = 10
                for i in range(n_clusters):
                    # Aggregate the silhouette scores for samples belonging to
                    # cluster i, and sort them
                    ith_cluster_silhouette_values = \
                        sample_silhouette_values[seg_cluster_labels == i]
    
                    ith_cluster_silhouette_values.sort()
                    thesilclusterstats[segment, 0, i] = np.mean(ith_cluster_silhouette_values)
                    thesilclusterstats[segment, 1, i] = np.median(ith_cluster_silhouette_values)
                    thesilclusterstats[segment, 2, i] = ith_cluster_silhouette_values[0]
                    thesilclusterstats[segment, 3, i] = ith_cluster_silhouette_values[-1]
    
                    size_cluster_i = ith_cluster_silhouette_values.shape[0]
    
                    if display:
                        y_upper = y_lower + size_cluster_i
                        color = cm.spectral(float(i) / n_clusters)
                        ax1.fill_betweenx(np.arange(y_lower, y_upper),
                                      0, ith_cluster_silhouette_values,
                                      facecolor=color, edgecolor=color, alpha=0.7)
    
                        # Label the silhouette plots with their cluster numbers at the middle
                        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
    
                        # Compute the new y_lower for next plot
                        y_lower = y_upper + 10  # 10 for the 0 samples
    
                if display:
                    ax1.set_title("The silhouette plot for the various clusters.")
                    ax1.set_xlabel("The silhouette coefficient values")
                    ax1.set_ylabel("Cluster label")
    
                    # The vertical line for average silhouette score of all the values
                    ax1.axvline(x=thesilavgs[segment], color="red", linestyle="--")
    
                    ax1.set_yticks([])  # Clear the yaxis labels / ticks
                    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])
                    plt.suptitle(("Silhouette analysis for KMeans clustering on sample data "
                              "with n_clusters = %d" % n_clusters),
                             fontsize=14, fontweight='bold')
    
                    plt.show()
        else:
            print('states are not fully populated - skipping stats')
    return thesilavgs, thesilclusterstats


def usage():
    print("")
    print("capfromtcs - calculate and cluster coactivation patterns for a set of timecourses")
    print("")
    print("usage: capfromtcs -i timecoursefile -o outputfile --samplefreq=FREQ --sampletime=TSTEP")
    print("                  [--nodetrend] [-s STARTTIME] [-D DURATION]")
    print("                  [-F LOWERFREQ,UPPERFREQ[,LOWERSTOP,UPPERSTOP]] [-V] [-L] [-R] [-C]")
    print("                  [-m] [-n NUMCLUSTER] [-b BATCHSIZE] [-S SEGMENTSIZE] [-I INITIALIZATIONS]")
    print("                  [--nonorm] [--pctnorm] [--varnorm] [--stdnorm] [--ppnorm] [--quality]")
    print("")
    print("required arguments:")
    print("    -i, --infile=TIMECOURSEFILE  - text file mulitple timeseries")
    print("    -o, --outfile=OUTNAME        - the root name of the output files")
    print("")
    print("    --samplefreq=FREQ            - sample frequency of all timecourses is FREQ ")
    print("           or")
    print("    --sampletime=TSTEP           - time step of all timecourses is TSTEP ")
    print("                                   NB: --samplefreq and --sampletime are two ways to specify")
    print("                                   the same thing.")
    print("")
    print("optional arguments:")
    print("    --nodetrend                  - do not detrend the data before correlation")
    print("    -s STARTTIME                 - time of first datapoint to use in seconds in the first file")
    print("    -D DURATION                  - amount of data to use in seconds")
    print("    -F                           - filter data and regressors from LOWERFREQ to UPPERFREQ.")
    print(
        "                                   LOWERSTOP and UPPERSTOP can be specified, or will be calculated automatically")
    print("    -V                           - filter data and regressors to VLF band")
    print("    -L                           - filter data and regressors to LFO band")
    print("    -R                           - filter data and regressors to respiratory band")
    print("    -C                           - filter data and regressors to cardiac band")
    print(
        '    -m                           - run MiniBatch Kmeans rather than conventional - use with very large datasets')
    print('    -n NUMCLUSTER                - set the number of clusters to NUMCLUSTER (default is 8)')
    print(
        '    -b BATCHSIZE                 - use a batchsize of BATCHSIZE if doing MiniBatch - ignored if not.  Default is 1000')
    print(
        '    -S SEGMENTSIZE               - treat the timecourses as segments of length SEGMENTSIZE for preprocessing.')
    print('                                   Default segmentsize is the entire length')
    print('    -I INITIALIZATIONS           - Restart KMeans INITIALIZATIONS times to find best fit (default is 1000)')
    print('    --nonorm                     - don\'t normalize timecourses')
    print('    --pctnorm                    - scale each timecourse to it\'s percentage of the mean')
    print('    --varnorm                    - scale each timecourse to have a variance of 1.0 (default)')
    print('    --stdnorm                    - scale each timecourse to have a standard deviation of 1.0')
    print('    --ppnorm                     - scale each timecourse to have a peak to peak range of 1.0')
    print('    --quality                    - perform a silhouette test to evaluate fit quality')
    print('    -v                           - turn on verbose mode')
    print('    --dbscan                     - perform dbscan clustering')
    print('    --pca                        - perform PCA analysis')
    print('    --ica                        - perform ICA analysis')
    print('    --GBR                        - apply gradient boosting regressor testing on clusters')
    print('    -d                           - display some quality metrics')
    print("")
    return ()


# get the command line parameters
summaryonly = True
dodetrend = True
normmethod = 'varnorm'
minibatch = False
n_clusters = 8
max_iter = 250
n_init = 100
batch_size = 1000
duration = 1000000.0
starttime = 0.0
usebutterworthfilter = False
filtorder = 3
verbose = False
analysistype = 'kmeans'
doGBR = False
display = False

# scan the command line arguments
try:
    opts, args = getopt.gnu_getopt(sys.argv[1:], "di:o:s:D:F:S:VLRCmn:b:I:v", ["infile=", "outfile=",
                                                                              "nodetrend",
                                                                              "dbscan",
                                                                              "GBR",
                                                                              "pca",
                                                                              "ica",
                                                                              "nonorm",
                                                                              "pctnorm",
                                                                              "varnorm",
                                                                              "stdnorm",
                                                                              "ppnorm",
                                                                              "quality",
                                                                              "samplefreq=", "sampletime=", "help"])
except getopt.GetoptError as err:
    # print help information and exit:
    print(str(err))  # will print something like "option -x not recognized"
    usage()
    sys.exit(2)

if len(args) > 1:
    print('capfromtcs takes no unflagged arguments')
    usage()
    exit()

# unset all required arguments
infilename = []
segsize = -1
sampletime = None
Fs = None
outfilename = None

theprefilter = tide.noncausalfilter()
theprefilter.setbutter(usebutterworthfilter, filtorder)

# set the default characteristics
theprefilter.settype('none')

for o, a in opts:
    if o == '--infile' or o == '-i':
        infilename.append(a)
        if verbose:
            print('will use', infilename[-1], 'as an input file')
    elif o == '--outfile' or o == '-o':
        outfilename = a
        if verbose:
            print('will use', outfilename, 'as output file')
    elif o == '-S':
        segsize = int(a)
        if verbose:
            print('Setting segment size to ', segsize)
    elif o == '--samplefreq':
        Fs = float(a)
        sampletime = 1.0 / Fs
        linkchar = '='
        if verbose:
            print('Setting sample frequency to ', Fs)
    elif o == '--sampletime':
        sampletime = float(a)
        Fs = 1.0 / sampletime
        linkchar = '='
        if verbose:
            print('Setting sample time step to ', sampletime)
    elif o == "-display":
        display = True
        if verbose:
            print('will display quality metrics')
    elif o == "--nonorm":
        normmethod = 'none'
        if verbose:
            print('will do no normalization')
    elif o == "--pctnorm":
        normmethod = 'pctnorm'
        if verbose:
            print('will do percent normalization')
    elif o == "--stdnorm":
        normmethod = 'stdnorm'
        if verbose:
            print('will do std dev normalization')
    elif o == "--varnorm":
        normmethod = 'varnorm'
        if verbose:
            print('will do variance normalization')
    elif o == "--ppnorm":
        normmethod = 'ppnorm'
        if verbose:
            print('will do p-p normalization')
    elif o == "--quality":
        summaryonly = False
        if verbose:
            print('will do silhouette test')
    elif o == "-v":
        verbose = True
        if verbose:
            print('verbose mode enabled')
    elif o == "--ica":
        analysistype = 'ica'
        if verbose:
            print('switching to ica analysis')
    elif o == "--GBR":
        doGBR = True
        if verbose:
            print('will do GBR on clusters')
    elif o == "--pca":
        analysistype = 'pca'
        if verbose:
            print('switching to pca analysis')
    elif o == "--dbscan":
        analysistype = 'dbscan'
        if verbose:
            print('switching to dbscan clustering')
    elif o == "--nodetrend":
        dodetrend = False
        if verbose:
            print('disabling detrending')
    elif o == "-D":
        duration = float(a)
        if verbose:
            print('duration set to', duration)
    elif o == "-s":
        starttime = float(a)
        if verbose:
            print('starttime set to', starttime)
    elif o == "-V":
        theprefilter.settype('vlf')
        if verbose:
            print('prefiltering to vlf band')
    elif o == "-L":
        theprefilter.settype('lfo')
        if verbose:
            print('prefiltering to lfo band')
    elif o == "-R":
        theprefilter.settype('resp')
        if verbose:
            print('prefiltering to respiratory band')
    elif o == "-C":
        theprefilter.settype('cardiac')
        if verbose:
            print('prefiltering to cardiac band')
    elif o == "-F":
        arbvec = a.split(',')
        if len(arbvec) != 2 and len(arbvec) != 4:
            usage()
            sys.exit()
        if len(arbvec) == 2:
            arb_lower = float(arbvec[0])
            arb_upper = float(arbvec[1])
            arb_lowerstop = 0.9 * float(arbvec[0])
            arb_upperstop = 1.1 * float(arbvec[1])
        if len(arbvec) == 4:
            arb_lower = float(arbvec[0])
            arb_upper = float(arbvec[1])
            arb_lowerstop = float(arbvec[2])
            arb_upperstop = float(arbvec[3])
        theprefilter.settype('arb')
        theprefilter.setarb(arb_lowerstop, arb_lower, arb_upper, arb_upperstop)
        if verbose:
            print('prefiltering to ', arb_lower, arb_upper, "(stops at ", arb_lowerstop, arb_upperstop, ")")
    elif o == "-m":
        minibatch = True
        print('will perform MiniBatchKMeans')
    elif o == "-b":
        batch_size = int(a)
        print('will use', batch_size, 'as batch_size')
    elif o == "-I":
        n_init = int(a)
        print('will do', n_init, 'initializations')
    elif o == "-n":
        n_clusters = int(a)
        print('will use', n_clusters, 'clusters')
    else:
        assert False, "unhandled option"

# check that required arguments are set
if outfilename is None:
    print('outfile must be set')
    usage()
    sys.exit()

if sampletime is None:
    print('sampletime must be set')
    usage()
    sys.exit()

if normmethod == 'none':
    print('will not normalize timecourses')
elif normmethod == 'pctnorm':
    print('will normalize timecourses to percentage of mean')
elif normmethod == 'stdnorm':
    print('will normalize timecourses to standard deviation of 1.0')
elif normmethod == 'varnorm':
    print('will normalize timecourses to variance of 1.0')
elif normmethod == 'ppnorm':
    print('will normalize timecourses to p-p deviation of 1.0')

# save the command line
tide.writevec([' '.join(sys.argv)], outfilename + '_commandline.txt')

# read in the files and get everything trimmed to the right length
startpoint = max([int(starttime * Fs), 0])
if len(infilename) == 1:
    # each column is a timecourse, each row is a timepoint
    matrixoutput = True
    inputdata = tide.readvecs(infilename[0])
    if verbose:
        print('input data shape is ', inputdata.shape)
    numpoints = inputdata.shape[1]
    endpoint = min([startpoint + int(duration * Fs), numpoints])
    trimmeddata = inputdata[:, startpoint:endpoint]
elif len(infilename) == 2:
    inputdata1 = tide.readvec(infilename[0])
    numpoints = len(inputdata1)
    inputdata2 = tide.readvec(infilename[1])
    endpoint1 = min([startpoint + int(duration * Fs), int(len(inputdata1)), int(len(inputdata2))])
    endpoint2 = min([int(duration * Fs), int(len(inputdata1)), int(len(inputdata2))])
    trimmeddata = np.zeros((2, numpoints), dtype='float')
    trimmeddata[0, :] = inputdata1[startpoint:endpoint1]
    trimmeddata[1, :] = inputdata2[0:endpoint2]
else:
    print('showstxcorr requires 1 multicolumn timecourse file or two single column timecourse files as input')
    usage()
    sys.exit()

# band limit the regressors if that is needed
if theprefilter.gettype() != 'none':
    if verbose:
        print("filtering to ", theprefilter.gettype(), " band")

thedims = trimmeddata.shape
n_features = thedims[0]
n_samples = thedims[1]
if segsize < 0:
    segsize = n_samples
print('input dataset has', n_features, 'features and', n_samples, 'samples in segments of size', segsize)
reformdata = np.reshape(trimmeddata, (n_features, n_samples))
if n_samples % segsize > 0:
    print('segment size is not an even divisor of the total length - exiting')
    sys.exit()
else:
    numsegs = int(n_samples // segsize)

for feature in range(n_features):
    if verbose:
        print('preprocessing feature', feature)
    for segment in range(numsegs):
        segstart = segment * segsize
        if dodetrend:
            segdata = tide.detrend(reformdata[feature, segstart:segstart + segsize])
        else:
            segdata = reformdata[feature, segstart:segstart + segsize]

        if normmethod == 'none':
            segnorm = segdata - np.mean(segdata)
        elif normmethod == 'pctnorm':
            segnorm = tide.pcnormalize(segdata)
        elif normmethod == 'varnorm':
            segnorm = tide.varnormalize(segdata)
        elif normmethod == 'stdnorm':
            segnorm = tide.stdnormalize(segdata)
        elif normmethod == 'ppnorm':
            segnorm = tide.ppnormalize(segdata)
        else:
            segnorm = segdata

        reformdata[feature, segstart:segstart + segsize] = theprefilter.apply(Fs, segnorm)
tide.writenpvecs(reformdata, outfilename + '_normalized.txt')
X = np.nan_to_num(np.transpose(reformdata))

if analysistype == 'kmeans':
    print('setting up kmeans')
    if minibatch:
        kmeans = MiniBatchKMeans(n_clusters=n_clusters, batch_size=batch_size, max_iter=max_iter).fit(X)
    else:
        kmeans = KMeans(n_clusters=n_clusters, max_iter=max_iter, n_init=n_init).fit(X)
    
    theclusters = np.transpose(kmeans.cluster_centers_)
    thestatelabels = kmeans.labels_
    print('thestatelabels shape', thestatelabels.shape)
    print('kmeans done')
    tide.writenpvecs(theclusters, outfilename + '_clustercenters.txt')
    tide.writenpvecs(thestatelabels, outfilename + '_statelabels.txt')

    # find most important features
    print('finding most important features')
    #rfe = RFE(kmeans, 10)
    #rfe.fit(X, thestatelabels)
    #print(rfe.support_)
    #print(rfe.ranking_)

    print('calling SelectPercentiles with X and y of dimensions',X.shape, thestatelabels.shape)
    selector = SelectPercentile(f_classif, percentile=10)
    selector.fit(X, thestatelabels)
    print(selector.get_params())
    X_indices = np.arange(X.shape[-1])
    scores = -np.nan_to_num(np.log10(np.nan_to_num(selector.pvalues_)))
    scores /= scores.max()
    sortedscores = np.sort(np.nan_to_num(selector.scores_))[::-1]
    print(sortedscores)
    if display:
        plt.bar(X_indices - .45, scores, width=.2,
            label=r'Univariate score ($-Log(p_{value})$)', color='darkorange')
        print(selector.get_support(indices=True))
        fig = plt.subplots(1, 1)
        plt.plot(sortedscores)
        plt.show()

    # now do some stats!
    thesilavgs, thesilclusterstats = silhouette_test(X, kmeans, n_clusters, numsegs, segsize, summaryonly)
    tide.writenpvecs(thesilavgs, outfilename + '_silhouettesegmentstats.txt')

    for segment in range(numsegs):
        thesestatelabels = thestatelabels[segment * segsize:(segment + 1) * segsize]

        outputaffine = np.eye(4)
        rawtransmat, thestats = statestats(thesestatelabels, n_clusters, 0)
        normtransmat = 1.0 * rawtransmat
        for i in range(n_clusters):
            if np.sum(rawtransmat[i, :]) > 0.0:
                normtransmat[i, :] /= np.sum(rawtransmat[i, :])
        offdiagtransmat = 1.0 * rawtransmat
        for i in range(n_clusters):
            offdiagtransmat[i, i] = 0.0
            if np.sum(offdiagtransmat[i, :]) > 0.0:
                offdiagtransmat[i, :] /= np.sum(offdiagtransmat[i, :])
        init_img=nib.Nifti1Image(normtransmat, outputaffine)
        init_hdr=init_img.get_header()
        init_sizes=init_hdr['pixdim']
        tide.savetonifti(rawtransmat, init_hdr, init_sizes, outfilename + '_seg_' + str(segment).zfill(4) + '_rawtransmat')
        tide.savetonifti(normtransmat, init_hdr, init_sizes, outfilename + '_seg_' + str(segment).zfill(4) + '_normtransmat')
        tide.savetonifti(offdiagtransmat, init_hdr, init_sizes, outfilename + '_seg_' + str(segment).zfill(4) + '_offdiagtransmat')
        # rawtransmat files are an n_clusters by n_clusters matrix with the total number of transitions from each state to each other state.
        # normtransmat files are an n_clusters by n_clusters matrix with the total for of transitions from each state to each other state.

        tide.writenpvecs(np.transpose(thestats), outfilename + '_seg_' + str(segment).zfill(4) + '_statestats.txt')
        thetimestats = 1.0 * thestats
        thetimestats[:, 2:] *= sampletime
        tide.writenpvecs(np.transpose(thetimestats), outfilename + '_seg_' + str(segment).zfill(4) + '_statetimestats.txt')

        tide.writenpvecs(thesestatelabels, outfilename + '_seg_' + str(segment).zfill(4) + '_statelabels.txt')
        print('Segment %d average silhouette Coefficient: %0.3f'
              % (segment, thesilavgs[segment]))
        for state in range(n_clusters):
            tc = np.where(thesestatelabels == state, 1, 0)
            tide.writenpvecs(tc,
                             outfilename + '_seg_' + str(segment).zfill(4) + '_instate_' + str(state).zfill(2) + '.txt')
        tide.writenpvecs(thesilclusterstats[segment, :, :],
                         outfilename + '_seg_' + str(segment).zfill(4) + '_silhouetteclusterstats.txt')

    if doGBR:
        clf = GradientBoostingRegressor().fit(X, thestatelabels)
        print('GBR fitting score is:', clf.score(X, thestatelabels))
        tide.writenpvecs(np.reshape(clf.feature_importances_, (n_features, 1)), outfilename + '_featureimportances.txt')

elif analysistype == 'ica':
    print('running FastICA')
    ica = FastICA(n_components=n_clusters, algorithm='deflation').fit(X)
    tide.writenpvecs(ica.components_, outfilename + "_icacomponents.txt")

elif analysistype == 'pca':
    print('running PCA')
    if minibatch:
        pca = IncrementalPCA(batch_size=batch_size, n_components=n_clusters).fit(X)
    else:
        pca = PCA(n_components=n_clusters).fit(X)
    for i in range(n_clusters):
        print('component', i, 'explained variance:', pca.explained_variance_[i], 'explained variance %:',
              100.0 * pca.explained_variance_ratio_[i])
    tide.writenpvecs(pca.components_, outfilename + "_pcacomponents.txt")

else:  # DBSCAN
    print('setting up DBSCAN')
    db = DBSCAN(eps=100.0, min_samples=5).fit(X)
    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
    core_samples_mask[db.core_sample_indices_] = True
    labels = db.labels_

    # Number of clusters in labels, ignoring noise if present.
    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)

    print('Estimated number of clusters: %d' % n_clusters_)
    # print("Homogeneity: %0.3f" % metrics.homogeneity_score(labels_true, labels))
    # print("Completeness: %0.3f" % metrics.completeness_score(labels_true, labels))
    # print("V-measure: %0.3f" % metrics.v_measure_score(labels_true, labels))
    # print("Adjusted Rand Index: %0.3f"
    # % metrics.adjusted_rand_score(labels_true, labels))
    # print("Adjusted Mutual Information: %0.3f"
    # % metrics.adjusted_mutual_info_score(labels_true, labels))
    print("Silhouette Coefficient: %0.3f"
          % metrics.silhouette_score(X, labels))
