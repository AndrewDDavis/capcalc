#!/usr/bin/env python
#
#   Copyright 2016 Blaise Frederick
#
#   Licensed under the Apache License, Version 2.0 (the "License");
#   you may not use this file except in compliance with the License.
#   You may obtain a copy of the License at
#
#       http://www.apache.org/licenses/LICENSE-2.0
#
#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS,
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#   See the License for the specific language governing permissions and
#   limitations under the License.
#
#
#       $Author: frederic $
#       $Date: 2016/06/14 12:04:50 $
#       $Id: linfit,v 1.4 2016/06/14 12:04:50 frederic Exp $
#
from __future__ import print_function, division
import sys
import getopt
import string
import platform
import rapidtide.tide_funcs as tide
import os

import numpy as np
from pylab import *
import nibabel as nib
from sklearn.feature_extraction.image import img_to_graph
from sklearn import metrics
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.semi_supervised import LabelPropagation
from sklearn.tree import DecisionTreeClassifier
from sklearn.manifold import TSNE
from sklearn.neighbors import KNeighborsClassifier
from sklearn.decomposition import FastICA, PCA, SparsePCA
from sklearn.preprocessing import StandardScaler
import scipy.sparse as ss
from matplotlib import pyplot as plt
from scipy.cluster.hierarchy import dendrogram

try:
    import hdbscan as hdbs
    hdbpresent = True
    print('hdbscan is present')
except:
    hdbpresent = False

def plot_dendrogram(model, **kwargs):

    # Children of hierarchical clustering
    children = model.children_

    # Distances between each pair of children
    # Since we don't have this information, we can use a uniform one for plotting
    distance = np.arange(children.shape[0])

    # The number of observations contained in each cluster level
    no_of_observations = np.arange(2, children.shape[0]+2)

    # Create linkage matrix and then plot the dendrogram
    linkage_matrix = np.column_stack([children, distance, no_of_observations]).astype(float)

    # Plot the corresponding dendrogram
    dendrogram(linkage_matrix, **kwargs)


def save_sparse_csr(filename,array):
    np.savez(filename,data = array.data ,indices=array.indices,
             indptr =array.indptr, shape=array.shape )


def load_sparse_csr(filename):
    loader = np.load(filename)
    return ss.csr_matrix((  loader['data'], loader['indices'], loader['indptr']),
                         shape = loader['shape'])


def N2one(loc, strides):
    retval = 0
    for i in range(len(loc) - 1):
        retval += loc[i] * strides[1 - i]
    retval += loc[-1]
    return retval


def three2one(loc, strides):
    return loc[0] * strides[1] + loc[1] * strides[0] + loc[2]


def one2N(index, strides):
    coords = []
    localindex = index + 0
    for i in range(len(strides)):
        coords.append(int(np.floor(localindex / strides[i - 1])))
        localindex -= coords[-1] * strides[i - 1]
    coords.append(np.mod(localindex, strides[0]))
    return coords


def one2three(index, strides):
    x = int(np.floor(index / strides[1]))
    y = int(np.floor((index - x * strides[1]) / strides[0]))
    z = int(np.mod(index, strides[0]))
    return [x, y, z]


def mkconnectivity(ptlist, radius, theshape, dodiag=False, dims=None):
    # convert the 1d index list to Nd locations
    ndmods = [theshape[-1]]
    for i in range(1, len(theshape) - 1):
        ndmods.append(theshape[-1 - i] * ndmods[i - 1])
    ptlistconv = []
    if len(theshape) != 3:
        # special case, 3d array
        for thepoint in ptlist:
            ptlistconv.append(one2three(thepoint, ndmods))
    else:
        for thepoint in ptlist:
            ptlistconv.append(one2N(thepoint, ndmods))
    
    # now make the connectivity matrix for nearest neighbors with given radius
    print('len ptlist =',len(ptlist))
    sm = ss.lil_matrix((len(ptlist), len(ptlist)), dtype='int')

    # sanity check first
    rangeinpts = int(np.floor(radius))
    if rangeinpts < 1:
        print('radius too small - no points in connectivity matrix')
        sys.exit()

    # now iterate over every pair
    nonzeroelems = 0
    radsq = radius * radius
    ptlistnp = np.asarray(ptlistconv)
    reportstep = 1000
    checkpointstep = 20000
    for ridx in range(len(ptlist)):
        if ridx % reportstep == 0:
            print('row:', ridx, ', nonzero elements:', nonzeroelems, ', loc:', ptlistconv[ridx])
        if ridx % checkpointstep == 0:
            print('checkpoint file')
            save_sparse_csr('connectivity_checkpoint', sm.tocsr())
        distarray = np.sum(np.square(ptlistnp[:, :] - ptlistnp[ridx, :]), axis=1)
        print
        matchlocs = np.where(distarray < radsq)[0]
        sm[ridx,matchlocs] = 1
        nonzeroelems += len(matchlocs)
    print(nonzeroelems, 'nonzero elements')

    # now stuff this into a sparse matrix
    #sm = ss.lil_matrix((len(ptlist), len(ptlist)), dtype='int')
    #for thepair in pairlist:
    #    sm[thepair[0], thepair[1]] = 1
    return sm.tocsr()


def usage():
    print("usage: supercluster datafile classes outputroot")
    print("")
    print("required arguments:")
    print("    datafile      - the name of the 4 dimensional nifti file to cluster")
    print("    classes       - the name of the 3 dimensional nifti file of the class labels")
    print("    outputroot    - the root name of the output nifti files")
    print("")
    print("optional arguments:")
    print("    --dmask=DATAMASK           - use DATAMASK to specify which voxels in the data to classify")
    print('    --n_estimators=ESIMATORS   - set ESTIMATORS (default is 10)')
    print('    --n_neighbors=NEIGHBORS    - use NEIGHBORS (default is 5)')
    print('    --type=CLASSIFIERTYPE      - set the classifier type (options are randomforest, svm,')
    print('                                 gradientboost. Default is randomforest)')
    print('    --min_samples=MINSAMPLES   - set min_samples to MINSAMPLES')
    print('    --affinity=AFFINITY        - set affinity to AFFINITY')
    print('    --linkage=LINKAGE          - set linkage to LINKAGE')
    print('    --radius=RADIUS            - set connectivity radius to RADIUS')
    print('    --noconn                   - do not use a connectivity matrix')
    print('    --display                  - display a 2d representation of the input data')
    print("")
    return()


# set default variable values
usedmask = False
classifiertype = 'randomforest'
display = False
doscale = False

# random forest variables
n_estimators = 20


# knn variables
weights = 'distance'
n_neighbors = 5

# parse command line arguments
try:
    opts, args = getopt.gnu_getopt(sys.argv, 'h', ["help",
                                                    "linkage=",
                                                    "display",
                                                    "prescale",
                                                    "dmask=",
                                                    "radius=",
                                                    "eps=",
                                                    "noconn",
                                                    "min_samples=",
                                                    "affinity=",
                                                    "n_estimators=",
                                                    "n_neighbors=",
                                                    "type="])
except getopt.GetoptError as err:
    # print(help information and exit:
    print(str(err)) # will print something like "option -a not recognized"
    usage()
    sys.exit(2)

# handle required args first
if len(args) < 4:
    print('spatial fit has 3 required arguments - ', len(args) - 1, 'found')
    usage()
    sys.exit()

datafilename=args[1]
classfilename=args[2]
outputrootname=args[3]

for o, a in opts:
    if o == "--n_estimators":
        n_estimators = int(a)
        print('will use', n_estimators, 'estimators')
    elif o == "--n_neighbors":
        n_neighbors = int(a)
        print('will use n_neighbors of', n_neighbors)
    elif o == "--dmask":
        usedmask = True
        datamaskname = a
        print('using', datamaskname, 'as data mask')
    elif o == "--radius":
        radius = float(a)
        print('will use connectivity radius of', radius)
    elif o == "--eps":
        eps = float(a)
        print('will use eps of', eps)
    elif o == "--min_samples":
        min_samples = int(a)
        print('will use min_samples of', min_samples)
    elif o == "--prescale":
        doscale = True
        print('will prescale data')
    elif o == "--linkage":
        linkage = a
        print('will use linkage', linkage)
    elif o == "--affinity":
        affinity = a
        print('will use affinity', affinity)
    elif o == "--type":
        classifiertype = a
        if classifiertype != 'randomforest' and \
            classifiertype != 'knn' and \
            classifiertype != 'labelprop' and \
            classifiertype != 'adaboost' and \
            classifiertype != 'gradientboost':
            print('illegal classifier mode - must be randomforest, knn, labelprop, adaboost, or gradientboost')
            sys.exit()
    elif o in ("-h", "--help"):
        usage()
        sys.exit()
    else:
        assert False, "unhandled option"


print('Will perform', classifiertype, 'classification')
    
# read in data
print("reading in data arrays")
datafile_img, datafile_data, datafile_hdr, datafiledims, datafilesizes = tide.readfromnifti(datafilename)
classfile_img, classfile_data, classfile_hdr, classfiledims, classfilesizes = tide.readfromnifti(classfilename)
if usedmask:
    datamask_img, datamask_data, datamask_hdr, datamaskdims, datamasksizes = tide.readfromnifti(datamaskname)

xsize, ysize, numslices, timepoints = tide.parseniftidims(datafiledims)
xdim, ydim, slicethickness, tr = tide.parseniftisizes(datafilesizes)
    
# check dimensions
print("checking class dimensions")
if not tide.checkspacematch(datafiledims, classfiledims):
    print('input mask spatial dimensions do not match image')
    exit()
if not classfiledims[4] == 1:
    print('class file must have time dimension of 1')
    exit()

if usedmask:
    print("checking mask dimensions")
    if not tide.checkspacematch(datafiledims, datamaskdims):
        print('input mask spatial dimensions do not match image')
        exit()
    if not datamaskdims[4] == 1:
        print('input mask time must have time dimension of 1')
        exit()

# save the command line
tide.writevec([' '.join(sys.argv)], outputrootname + '_commandline.txt')

# allocating arrays
print("reshaping arrays")
numspatiallocs = int(xsize) * int(ysize) * int(numslices)
rs_datafile = datafile_data.reshape((numspatiallocs, timepoints))
rs_classfile = classfile_data.reshape((numspatiallocs))
trainlocs = np.where(rs_classfile > 0)

print("masking arrays")
if usedmask:
    proclocs = np.where(datamask_data.reshape((numspatiallocs)) > 0.9)
else:
    datamaskdims = [1, xsize, ysize, numslices, 1]
    themaxes = np.max(rs_datafile, axis=1)
    themins = np.min(rs_datafile, axis=1)
    thediffs = (themaxes - themins).reshape((numspatiallocs))
    proclocs = np.where(thediffs > 0.0)
traindata = rs_datafile[trainlocs, :][0]
procdata = rs_datafile[proclocs, :][0]
y = rs_classfile[trainlocs]
print(rs_datafile.shape, traindata.shape, procdata.shape, y.shape)

# Scale the data
if doscale:
    X_train = StandardScaler().fit_transform(traindata)
    X_proc = StandardScaler().fit_transform(procdata)
else:
    X_train = traindata
    X_proc = procdata

# take a look at it
if display:
    projection = TSNE().fit_transform(X_train)
    plt.scatter(*projection.T, **plot_kwds)
    plt.show()

# set up the classifier
if classifiertype == 'randomforest':
    theclassifier = RandomForestClassifier(n_estimators=n_estimators, max_depth=None,
        min_samples_split=2, random_state=0, n_jobs=-1)
    methodname = 'randomforest_' + str(n_estimators).zfill(2)

#elif classifiertype == 'labelprop':
#    labelprop = AdaBoostClassifier(n_estimators=n_estimators)
#    labelprop.fit(X_train, y)
#    print('model fit complete')
#    scores = cross_val_score(labelprop, X_train, y)
#    print('score calculation complete')
#    print('scores mean:', scores.mean())
#    methodname = 'labelprop_' + weights + '_' + str(n_neighbors).zfill(2)
#    theregionlabels = labelprop.predict(X_proc)
#    print('prediction complete')

elif classifiertype == 'adaboost':
    ada = AdaBoostClassifier(n_estimators=n_estimators)
    methodname = 'ada_' + str(n_neighbors).zfill(2)

elif classifiertype == 'gradientboost':
    gbc = GradientBoostingClassifier(n_estimators=n_estimators)
    methodname = 'gbc_' + str(n_neighbors).zfill(2)

elif classifiertype == 'knn':
    knn = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights, n_jobs=-1)
    methodname = 'knn_' + weights + '_' + str(n_neighbors).zfill(2)

else:
    print('illegal classifier type')
    sys.exit()

# evaluate the accuracy
scores = cross_val_score(theclassifier, X_train, y, n_jobs=-1)
print('score calculation complete')
print('scores mean, min, max:', scores.mean(), scores.min(), scores.max())


# train on the training set
theclassifier.fit(X_train, y)
print('model fit complete')

# now classify the entire image
theregionlabels = theclassifier.predict(X_proc)
print('prediction complete')

# run other classifier-specific metrics
if classifiertype == 'randomforest' or classifiertype == 'adaboost' or classifiertype == 'gradientboost':
    plt.plot(theclassifier.feature_importances_)
    plt.show()

# save the data
theheader = datafile_hdr
theheader['dim'][4] = 1
tempout = np.zeros((numspatiallocs), dtype='float')
tempout[proclocs] = theregionlabels[:] + 1
tide.savetonifti(tempout.reshape((xsize, ysize, numslices, 1)), datafile_hdr, datafilesizes,
    outputrootname + '_' + methodname + '_regions')
