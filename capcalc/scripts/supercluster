#!/usr/bin/env python
#
#   Copyright 2016 Blaise Frederick
#
#   Licensed under the Apache License, Version 2.0 (the "License");
#   you may not use this file except in compliance with the License.
#   You may obtain a copy of the License at
#
#       http://www.apache.org/licenses/LICENSE-2.0
#
#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS,
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#   See the License for the specific language governing permissions and
#   limitations under the License.
#
#
#       $Author: frederic $
#       $Date: 2016/06/14 12:04:50 $
#       $Id: linfit,v 1.4 2016/06/14 12:04:50 frederic Exp $
#
from __future__ import print_function, division
import sys
import getopt
import string
import platform
import rapidtide.tide_funcs as tide
import os

import numpy as np
from pylab import *
import nibabel as nib
from sklearn.feature_extraction.image import img_to_graph
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier
from sklearn.semi_supervised import LabelPropagation
from sklearn.tree import DecisionTreeClassifier
from sklearn.manifold import TSNE
from sklearn.neighbors import KNeighborsClassifier
from sklearn.decomposition import FastICA, PCA, SparsePCA
from sklearn.preprocessing import StandardScaler, RobustScaler
import scipy.sparse as ss
from matplotlib import pyplot as plt
from scipy.cluster.hierarchy import dendrogram

try:
    import hdbscan as hdbs
    hdbpresent = True
    print('hdbscan is present')
except:
    hdbpresent = False

def plot_dendrogram(model, **kwargs):

    # Children of hierarchical clustering
    children = model.children_

    # Distances between each pair of children
    # Since we don't have this information, we can use a uniform one for plotting
    distance = np.arange(children.shape[0])

    # The number of observations contained in each cluster level
    no_of_observations = np.arange(2, children.shape[0]+2)

    # Create linkage matrix and then plot the dendrogram
    linkage_matrix = np.column_stack([children, distance, no_of_observations]).astype(float)

    # Plot the corresponding dendrogram
    dendrogram(linkage_matrix, **kwargs)


def save_sparse_csr(filename,array):
    np.savez(filename,data = array.data ,indices=array.indices,
             indptr =array.indptr, shape=array.shape )


def load_sparse_csr(filename):
    loader = np.load(filename)
    return ss.csr_matrix((  loader['data'], loader['indices'], loader['indptr']),
                         shape = loader['shape'])


def N2one(loc, strides):
    retval = 0
    for i in range(len(loc) - 1):
        retval += loc[i] * strides[1 - i]
    retval += loc[-1]
    return retval


def three2one(loc, strides):
    return loc[0] * strides[1] + loc[1] * strides[0] + loc[2]


def one2N(index, strides):
    coords = []
    localindex = index + 0
    for i in range(len(strides)):
        coords.append(int(np.floor(localindex / strides[i - 1])))
        localindex -= coords[-1] * strides[i - 1]
    coords.append(np.mod(localindex, strides[0]))
    return coords


def one2three(index, strides):
    x = int(np.floor(index / strides[1]))
    y = int(np.floor((index - x * strides[1]) / strides[0]))
    z = int(np.mod(index, strides[0]))
    return [x, y, z]


def mkconnectivity(ptlist, radius, theshape, dodiag=False, dims=None):
    # convert the 1d index list to Nd locations
    ndmods = [theshape[-1]]
    for i in range(1, len(theshape) - 1):
        ndmods.append(theshape[-1 - i] * ndmods[i - 1])
    ptlistconv = []
    if len(theshape) != 3:
        # special case, 3d array
        for thepoint in ptlist:
            ptlistconv.append(one2three(thepoint, ndmods))
    else:
        for thepoint in ptlist:
            ptlistconv.append(one2N(thepoint, ndmods))
    
    # now make the connectivity matrix for nearest neighbors with given radius
    print('len ptlist =',len(ptlist))
    sm = ss.lil_matrix((len(ptlist), len(ptlist)), dtype='int')

    # sanity check first
    rangeinpts = int(np.floor(radius))
    if rangeinpts < 1:
        print('radius too small - no points in connectivity matrix')
        sys.exit()

    # now iterate over every pair
    nonzeroelems = 0
    radsq = radius * radius
    ptlistnp = np.asarray(ptlistconv)
    reportstep = 1000
    checkpointstep = 20000
    for ridx in range(len(ptlist)):
        if ridx % reportstep == 0:
            print('row:', ridx, ', nonzero elements:', nonzeroelems, ', loc:', ptlistconv[ridx])
        if ridx % checkpointstep == 0:
            print('checkpoint file')
            save_sparse_csr('connectivity_checkpoint', sm.tocsr())
        distarray = np.sum(np.square(ptlistnp[:, :] - ptlistnp[ridx, :]), axis=1)
        print
        matchlocs = np.where(distarray < radsq)[0]
        sm[ridx,matchlocs] = 1
        nonzeroelems += len(matchlocs)
    print(nonzeroelems, 'nonzero elements')

    # now stuff this into a sparse matrix
    #sm = ss.lil_matrix((len(ptlist), len(ptlist)), dtype='int')
    #for thepair in pairlist:
    #    sm[thepair[0], thepair[1]] = 1
    return sm.tocsr()


def usage():
    print("usage: supercluster datafile classes outputroot")
    print("")
    print("required arguments:")
    print("    datafile      - the name of the 4 dimensional nifti file to cluster")
    print("    classes       - the name of the 3 dimensional nifti file of the class labels")
    print("    outputroot    - the root name of the output nifti files")
    print("")
    print("optional arguments:")
    print("    --dmask=DATAMASK            - use DATAMASK to specify which voxels in the data to classify")
    print('    --n_estimators=ESIMATORS    - set ESTIMATORS (default is 10)')
    print('    --n_neighbors=NEIGHBORS     - use NEIGHBORS (default is 5)')
    print('    --type=CLASSIFIERTYPE       - set the classifier type (options are randomforest, knn,')
    print('                                  gradientboost, adaboost, extratrees. Default is randomforest)')
    print('    --min_samples=MINSAMPLES    - set min_samples to MINSAMPLES')
    print('    --affinity=AFFINITY         - set affinity to AFFINITY')
    print('    --linkage=LINKAGE           - set linkage to LINKAGE')
    print('    --radius=RADIUS             - set connectivity radius to RADIUS')
    print('    --weights=WEIGHTS           - use WEIGHTS weighting for knn (options are uniform and distance (default))')
    print('    --noconn                    - do not use a connectivity matrix')
    print('    --display                   - display a 2d representation of the input data')
    print('    --prescale                  - prescale input data prior to clustering')
    print('    --bagging                   - construct a bagging classifier')
    print('    --scaleintervals=R1,R2,...  - apply scaling to ranges R1, R2,... independently')
    print('                                  NOTE: turns on prescaling')
    print("")
    return()


# set default variable values
usedmask = False
classifiertype = 'randomforest'
display = False
prescale = False
multirun = False
scaler = 'robust'
numtests = 10
intlist = None
bagging = True

# random forest variables
n_estimators = 20

# knn variables
weights = 'distance'
n_neighbors = 5

# parse command line arguments
try:
    opts, args = getopt.gnu_getopt(sys.argv, 'h', ["help",
                                                    "linkage=",
                                                    "scaleintervals=",
                                                    "display",
                                                    "prescale",
                                                    "dmask=",
                                                    "numtests=",
                                                    "multirun",
                                                    "radius=",
                                                    "weights=",
                                                    "eps=",
                                                    "noconn",
                                                    "min_samples=",
                                                    "affinity=",
                                                    "n_estimators=",
                                                    "n_neighbors=",
                                                    "type="])
except getopt.GetoptError as err:
    # print(help information and exit:
    print(str(err)) # will print something like "option -a not recognized"
    usage()
    sys.exit(2)

# handle required args first
if len(args) < 4:
    print('spatial fit has 3 required arguments - ', len(args) - 1, 'found')
    usage()
    sys.exit()

datafilename=args[1]
classfilename=args[2]
outputrootname=args[3]

for o, a in opts:
    if o == "--n_estimators":
        n_estimators = int(a)
        print('will use', n_estimators, 'estimators')
    elif o == "--n_neighbors":
        n_neighbors = int(a)
        print('will use n_neighbors of', n_neighbors)
    elif o == "--dmask":
        usedmask = True
        datamaskname = a
        print('using', datamaskname, 'as data mask')
    elif o == "--weights":
        radius = a
        print('will use ', weights, 'weighting')
    elif o == "--radius":
        radius = float(a)
        print('will use connectivity radius of', radius)
    elif o == "--numtests":
         numtests = int(a)
         print('will run', numtests, 'accuracy checks')
    elif o == "--eps":
        eps = float(a)
        print('will use eps of', eps)
    elif o == "--min_samples":
        min_samples = int(a)
        print('will use min_samples of', min_samples)
    elif o == "--multirun":
        multirun = True
        print('will run multiplle classifications')
    elif o == "--prescale":
        prescale = True
        print('will prescale data')
    elif o == "--scaleintervals":
        prescale = True
        intlist = list(map(int, a.split(',')))
        print('will use intervals', intlist)
    elif o == "--linkage":
        linkage = a
        print('will use linkage', linkage)
    elif o == "--affinity":
        affinity = a
        print('will use affinity', affinity)
    elif o == "--type":
        classifiertype = a
        if classifiertype != 'randomforest' and \
            classifiertype != 'knn' and \
            classifiertype != 'extratrees' and \
            classifiertype != 'adaboost' and \
            classifiertype != 'gradientboost':
            print('illegal classifier mode - must be randomforest, extratrees, knn, adaboost, or gradientboost')
            sys.exit()
    elif o in ("-h", "--help"):
        usage()
        sys.exit()
    else:
        assert False, "unhandled option"


print('Will perform', classifiertype, 'classification')
    
# read in data
print("reading in data arrays")
datafile_img, datafile_data, datafile_hdr, datafiledims, datafilesizes = tide.readfromnifti(datafilename)
classfile_img, classfile_data, classfile_hdr, classfiledims, classfilesizes = tide.readfromnifti(classfilename)
if usedmask:
    datamask_img, datamask_data, datamask_hdr, datamaskdims, datamasksizes = tide.readfromnifti(datamaskname)

xsize, ysize, numslices, timepoints = tide.parseniftidims(datafiledims)
xdim, ydim, slicethickness, tr = tide.parseniftisizes(datafilesizes)
    
# check dimensions
print("checking class dimensions")
if not tide.checkspacematch(datafiledims, classfiledims):
    print('input mask spatial dimensions do not match image')
    exit()
if not classfiledims[4] == 1:
    print('class file must have time dimension of 1')
    exit()

if usedmask:
    print("checking mask dimensions")
    if not tide.checkspacematch(datafiledims, datamaskdims):
        print('input mask spatial dimensions do not match image')
        exit()
    if not datamaskdims[4] == 1:
        print('input mask time must have time dimension of 1')
        exit()

# allocating arrays
print("reshaping arrays")
numspatiallocs = int(xsize) * int(ysize) * int(numslices)
rs_datafile = datafile_data.reshape((numspatiallocs, timepoints))
rs_classfile = classfile_data.reshape((numspatiallocs))
knownlocs = np.where(rs_classfile > 0)

print("masking arrays")
if usedmask:
    proclocs = np.where(datamask_data.reshape((numspatiallocs)) > 0.9)
else:
    datamaskdims = [1, xsize, ysize, numslices, 1]
    themaxes = np.max(rs_datafile, axis=1)
    themins = np.min(rs_datafile, axis=1)
    thediffs = (themaxes - themins).reshape((numspatiallocs))
    proclocs = np.where(thediffs > 0.0)

# construct the arrays 
y_known = rs_classfile[knownlocs]

X_known = rs_datafile[knownlocs, :][0]
X_predict = rs_datafile[proclocs, :][0]
print(rs_datafile.shape, X_known.shape, X_predict.shape, y_known.shape)

# Scale the data
if intlist is None:
    intlist = [timepoints]
if prescale:
    print('prescaling...')
    thepos = 0
    for interval in intlist:
        if scaler == 'standard':
            X_known[:, thepos:(thepos + interval)] = StandardScaler().fit_transform(X_known[:, thepos:(thepos + interval)])
            X_predict[:, thepos:(thepos + interval)] = StandardScaler().fit_transform(X_predict[:, thepos:(thepos + interval)])
        else:
            X_known[:, thepos:(thepos + interval)] = RobustScaler().fit_transform(X_known[:, thepos:(thepos + interval)])
            X_predict[:, thepos:(thepos + interval)] = RobustScaler().fit_transform(X_predict[:, thepos:(thepos + interval)])
        thepos += interval
    prescalename = 'prescale_' + scaler + '_'
    print('Done')
else:
    prescalename = ''

# take a look at it
if display and False:
    projection = TSNE().fit_transform(X_known)
    plt.scatter(*projection.T, **plot_kwds)
    plt.show()
    
# set up the classifier
if classifiertype == 'randomforest':
    if bagging:
        theclassifier = BaggingClassifier( \
            RandomForestClassifier( \
                n_estimators=n_estimators, \
                max_depth=None,
                min_samples_split=2, \
                n_jobs=-1),
            max_samples=0.5, max_features=0.5)
        methodname = 'randomforest_bagging_' + prescalename + str(n_estimators).zfill(2)
    else:
        theclassifier = RandomForestClassifier( \
            n_estimators=n_estimators, \
            max_depth=None,
            min_samples_split=2, \
            random_state=0, \
            n_jobs=-1)
        methodname = 'randomforest_' + prescalename + str(n_estimators).zfill(2)

elif classifiertype == 'extratrees':
    if bagging:
        theclassifier = BaggingClassifier( \
            ExtraTreesClassifier( \
                n_estimators=n_estimators, \
                max_depth=None,
                min_samples_split=2, \
                n_jobs=-1),
            max_samples=0.5, max_features=0.5)
        methodname = 'extratrees_bagging_' + prescalename + str(n_estimators).zfill(2)
    else:
        theclassifier = ExtraTreesClassifier( \
            n_estimators=n_estimators, \
            max_depth=None,
            min_samples_split=2, \
            random_state=0, \
            n_jobs=-1)
        methodname = 'extratrees_' + prescalename + str(n_estimators).zfill(2)

elif classifiertype == 'adaboost':
    theclassifier = AdaBoostClassifier(n_estimators=n_estimators)
    methodname = 'ada_' + prescalename + str(n_neighbors).zfill(2)

elif classifiertype == 'gradientboost':
    theclassifier = GradientBoostingClassifier(n_estimators=n_estimators)
    methodname = 'gbc_' + prescalename + str(n_neighbors).zfill(2)

elif classifiertype == 'knn':
    if bagging:
        theclassifier = BaggingClassifier( \
            KNeighborsClassifier( \
                n_neighbors=n_neighbors, \
                weights=weights, \
                n_jobs=-1),
            max_samples=0.5, max_features=0.5)
        methodname = 'knn_bagging_' + prescalename + weights + '_' + str(n_neighbors).zfill(2)
    else:
        theclassifier = KNeighborsClassifier( \
            n_neighbors=n_neighbors, \
            weights=weights, \
            n_jobs=-1)
        methodname = 'knn_' + prescalename + weights + '_' + str(n_neighbors).zfill(2)

else:
    print('illegal classifier type')
    sys.exit()

# check accuracy vs. number of components
ncomps = timepoints

# check the accuracy of the model
accuracies = []
if multirun:
    y_predict_all = np.zeros((X_predict.shape[0],numtests), dtype=np.float)
for testrun in range(numtests):
    X_train, X_test, y_train, y_test = train_test_split(X_known[:, 0:ncomps], y_known, test_size=0.2)
    theclassifier.fit(X_train, y_train)
   
    # now make predictions on the test data
    y_predict = theclassifier.predict(X_test)
    accuracies.append(accuracy_score(y_test, y_predict))

    # if we are doing multirun, make a prediction on all known data for each training
    if multirun:
        print('predicting all data for test', testrun)
        y_predict_all[:, testrun] = theclassifier.predict(X_predict)
  
if len(accuracies) > 0:
    theaccuracies = np.asarray(accuracies, dtype=np.float)
    print('Accuracies for', ncomps, 'components (mean, std, min, max):', \
        np.mean(theaccuracies), np.std(theaccuracies), np.min(theaccuracies), np.max(theaccuracies))
    with open(outputrootname + '_' + methodname + '_accuracy.csv', "w") as text_file:
        dpath, dfile = os.path.split(datafilename)
        cpath, cfile = os.path.split(classfilename)
        fieldlist = [dfile, cfile, methodname, str(np.mean(theaccuracies)), str(np.std(theaccuracies)), str(np.min(theaccuracies)), str(np.max(theaccuracies))]
        text_file.write(','.join(fieldlist) + '\n')

# now classify all the known data to calculate metrics
y_predict_known = theclassifier.predict(X_known)
print('prediction of known locations complete')
confusion = confusion_matrix(y_known, y_predict_known)
classreport = classification_report(y_known, y_predict_known)
tide.writenpvecs(confusion, outputrootname + '_' + methodname + '_predict_all_confusion_matrix.txt')
print(classreport)

if not multirun:
    # now classify the entire image
    y_predict_all = theclassifier.predict(X_predict)
    print('prediction of all locations complete')

# run other classifier-specific metrics
if display:
    if classifiertype == 'randomforest' or classifiertype == 'adaboost' or classifiertype == 'gradientboost':
        plt.plot(theclassifier.feature_importances_)
        plt.show()

# save the data
theheader = datafile_hdr
theheader['dim'][4] = 1

# output classification of all known data
tempout = np.zeros((numspatiallocs), dtype='float')
tempout[knownlocs] = y_predict_known[:]
tide.savetonifti(tempout.reshape((xsize, ysize, numslices, 1)), datafile_hdr, datafilesizes,
    outputrootname + '_' + methodname + '_predict_known')

# save the command line
tide.writevec([' '.join(sys.argv)], outputrootname + '_' + methodname + '_commandline.txt')

# output classification of all known data
if multirun:
    tempout = np.zeros((numspatiallocs, numtests), dtype='float')
    tempout[proclocs, :] = y_predict_all[:, :]
    theheader = datafile_hdr
    theheader['dim'][4] = numtests
    tide.savetonifti(tempout.reshape((xsize, ysize, numslices, numtests)), theheader, datafilesizes,
        outputrootname + '_' + methodname + '_predict_all')
else:
    tempout = np.zeros((numspatiallocs), dtype='float')
    tempout[proclocs] = y_predict_all[:]
    tide.savetonifti(tempout.reshape((xsize, ysize, numslices, 1)), datafile_hdr, datafilesizes,
        outputrootname + '_' + methodname + '_predict_all')
